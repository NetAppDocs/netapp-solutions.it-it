---
sidebar: sidebar 
permalink: vmware/vmware_vcf_asa_supp_wkld_nvme.html 
keywords: netapp, vmware, cloud, foundation, vcf, aff, all-flash, nfs, vvol, vvols, array, ontap tools, otv, sddc, iscsi 
summary:  
---
= Configurare lo storage supplementare NVMe/TCP per i domini del carico di lavoro VCF
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
In questo scenario, dimostreremo come configurare lo storage supplementare NVMe/TCP per un dominio di carico di lavoro VCF.

Autore: Josh Powell



== Panoramica dello scenario

Questo scenario copre i seguenti passaggi di alto livello:

* Crea una Storage Virtual Machine (SVM) con interfacce logiche (LIF) per il traffico NVMe/TCP.
* Creare gruppi di porte distribuite per le reti iSCSI nel dominio del carico di lavoro VI.
* Creare adattatori vmkernel per iSCSI sugli host ESXi per il dominio del carico di lavoro VI.
* Aggiungere adattatori NVMe/TCP sugli host ESXi.
* Implementa il datastore NVMe/TCP.




== Prerequisiti

Questo scenario richiede i seguenti componenti e configurazioni:

* Un sistema di storage ONTAP ASA con porte per dati fisici su switch ethernet dedicati al traffico di storage.
* La distribuzione del dominio di gestione VCF è stata completata e il client vSphere è accessibile.
* Un dominio del carico di lavoro VI è stato distribuito in precedenza.


NetApp consiglia design di rete completamente ridondanti per NVMe/TCP. Il diagramma seguente illustra un esempio di configurazione ridondante, che fornisce tolleranza agli errori per sistemi di archiviazione, switch, schede di rete e sistemi host. Consultare il NetApp link:https://docs.netapp.com/us-en/ontap/san-config/index.html["Riferimento alla configurazione SAN"] per ulteriori informazioni.

image:vmware-vcf-asa-image74.png["Progetto di rete NVMe-tcp"]

Per il multipathing e il failover su percorsi multipli, NetApp consiglia di disporre di un minimo di due LIF per nodo storage in reti ethernet separate per tutte le SVM nelle configurazioni NVMe/TCP.

Questa documentazione illustra il processo di creazione di una nuova SVM e specifica delle informazioni dell'indirizzo IP per creare LIF multipli per il traffico NVMe/TCP. Per aggiungere nuove LIF a una SVM esistente, fare riferimento a. link:https://docs.netapp.com/us-en/ontap/networking/create_a_lif.html["Creazione di una LIF (interfaccia di rete)"].

Per ulteriori informazioni sulle considerazioni sulla progettazione NVMe per i sistemi storage ONTAP, fare riferimento a. link:https://docs.netapp.com/us-en/ontap/nvme/support-limitations.html["Configurazione, supporto e limitazioni NVMe"].



== Fasi di implementazione

Per creare un datastore VMFS su un dominio di carico di lavoro VCF utilizzando NVMe/TCP, completa i seguenti passaggi.



=== Crea SVM, LIF e namespace NVMe su un sistema storage ONTAP

Il passaggio seguente viene eseguito in Gestione di sistema di ONTAP.

.Creazione di LIF e macchine virtuali storage
[%collapsible]
====
Completa i seguenti passaggi per creare una SVM insieme a LIF multipli per traffico NVMe/TCP.

. Da Gestione di sistema di ONTAP, accedere a *Storage VM* nel menu a sinistra e fare clic su *+ Aggiungi* per iniziare.
+
image:vmware-vcf-asa-image01.png["Fare clic su +Add (Aggiungi) per iniziare a creare la SVM"]

+
{nbsp}

. Nella procedura guidata *Add Storage VM* (Aggiungi VM di storage) fornire un *Name* (Nome) per la SVM, selezionare *IP Space* (spazio IP), quindi, in *Access Protocol* (protocollo di accesso), fare clic sulla scheda *NVMe* e selezionare la casella *Enable NVMe/TCP* (Abilita NVMe/TCP*).
+
image:vmware-vcf-asa-image75.png["Procedura guidata Aggiungi VM di storage - attiva NVMe/TCP"]

+
{nbsp}

. Nella sezione *interfaccia di rete* compilare i campi *indirizzo IP*, *Subnet Mask* e *Broadcast Domain and Port* per la prima LIF. Per LIF successive, la casella di controllo può essere abilitata per usare impostazioni comuni a tutte le LIF rimanenti o per usare impostazioni separate.
+

NOTE: Per il multipathing e il failover su percorsi multipli, NetApp consiglia di disporre di un minimo di due LIF per nodo storage in reti Ethernet separate per tutte le SVM nelle configurazioni NVMe/TCP.

+
image:vmware-vcf-asa-image76.png["Compila le informazioni di rete per le LIF"]

+
{nbsp}

. Scegliere se attivare l'account Storage VM Administration (per ambienti multi-tenancy) e fare clic su *Save* (Salva) per creare la SVM.
+
image:vmware-vcf-asa-image04.png["Attiva account SVM e fine"]



====
.Creare il namespace NVMe
[%collapsible]
====
I namespace NVMe sono analoghi alle LUN per iSCSI o FC. È necessario creare il namespace NVMe prima di poter implementare un datastore VMFS da vSphere Client. Per creare il namespace NVMe, occorre prima ottenere il NVMe Qualified Name (NQN) da ogni host ESXi nel cluster. L'NQN viene utilizzato da ONTAP per fornire il controllo dell'accesso allo spazio dei nomi.

Completare i seguenti passaggi per creare un namespace NVMe:

. Aprire una sessione SSH con un host ESXi nel cluster per ottenere il proprio NQN. Utilizzare il seguente comando dall'interfaccia CLI:
+
[source, cli]
----
esxcli nvme info get
----
+
Dovrebbe essere visualizzato un output simile al seguente:

+
[source, cli]
----
Host NQN: nqn.2014-08.com.netapp.sddc:nvme:vcf-wkld-esx01
----
. Registrare l'NQN per ciascun host ESXi nel cluster
. Da Gestione di sistema di ONTAP, accedere a *NVMe Namespaces* nel menu a sinistra e fare clic su *+ Aggiungi* per iniziare.
+
image:vmware-vcf-asa-image93.png["Fare clic su +Add (Aggiungi) per creare un namespace NVMe"]

+
{nbsp}

. Nella pagina *Add NVMe Namespace*, inserire un prefisso nome, il numero di namespace da creare, le dimensioni dello spazio dei nomi e il sistema operativo host che accederà allo spazio dei nomi. Nella sezione *host NQN* creare un elenco separato da virgole degli NQN precedentemente raccolti dagli host ESXi che accederanno agli spazi dei nomi.


Fare clic su *altre opzioni* per configurare elementi aggiuntivi come il criterio di protezione delle istantanee. Infine, fare clic su *Save* per creare lo spazio dei nomi NVMe.

+ image:vmware-vcf-asa-image93.png["Fare clic su +Add (Aggiungi) per creare un namespace NVMe"]

====


=== Configurare le schede di rete e il software NVMe sugli host ESXi

I seguenti passaggi vengono eseguiti sul cluster di dominio del carico di lavoro VI utilizzando il client vSphere. In questo caso viene utilizzato vCenter Single Sign-on, pertanto il client vSphere è comune sia ai domini di gestione che ai domini di workload.

.Creare gruppi di porte distribuite per il traffico NVME/TCP
[%collapsible]
====
Completare quanto segue per creare un nuovo gruppo di porte distribuite per ogni rete NVMe/TCP:

. Dal client vSphere , accedere a *Inventory > Networking* per il dominio del carico di lavoro. Passare allo Switch distribuito esistente e scegliere l'azione da creare *nuovo Gruppo di porte distribuite...*.
+
image:vmware-vcf-asa-image22.png["Scegliere di creare un nuovo gruppo di porte"]

+
{nbsp}

. Nella procedura guidata *nuovo gruppo di porte distribuite* inserire un nome per il nuovo gruppo di porte e fare clic su *Avanti* per continuare.
. Nella pagina *Configura impostazioni* completare tutte le impostazioni. Se si utilizzano VLAN, assicurarsi di fornire l'ID VLAN corretto. Fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-image23.png["Inserire l'ID VLAN"]

+
{nbsp}

. Nella pagina *Pronto per il completamento*, rivedere le modifiche e fare clic su *fine* per creare il nuovo gruppo di porte distribuite.
. Ripetere questa procedura per creare un gruppo di porte distribuite per la seconda rete NVMe/TCP in uso e assicurarsi di aver immesso il corretto *VLAN ID*.
. Una volta creati entrambi i gruppi di porte, accedere al primo gruppo di porte e selezionare l'azione *Modifica impostazioni...*.
+
image:vmware-vcf-asa-image77.png["DPG - consente di modificare le impostazioni"]

+
{nbsp}

. Nella pagina *Gruppo porte distribuite - Modifica impostazioni*, accedere a *Teaming and failover* nel menu a sinistra e fare clic su *uplink2* per spostarlo in basso in *uplink non utilizzati*.
+
image:vmware-vcf-asa-image78.png["spostare uplink2 su inutilizzato"]

. Ripetere questo passo per il secondo gruppo di porte NVMe/TCP. Tuttavia, questa volta si sposta *uplink1* verso il basso in *uplink non utilizzati*.
+
image:vmware-vcf-asa-image79.png["sposta uplink 1 in inutilizzato"]



====
.Creare adattatori VMkernel su ciascun host ESXi
[%collapsible]
====
Ripetere questo processo su ogni host ESXi nel dominio del carico di lavoro.

. Dal client vSphere, passare a uno degli host ESXi nell'inventario del dominio del carico di lavoro. Dalla scheda *Configure* selezionare *VMkernel adapters* e fare clic su *Add Networking...* per iniziare.
+
image:vmware-vcf-asa-image30.png["Avviare la procedura guidata di aggiunta della rete"]

+
{nbsp}

. Nella finestra *Select Connection type* (Seleziona tipo di connessione), scegliere *VMkernel Network Adapter* (scheda di rete VMkernel) e fare clic su *Next* (Avanti) per continuare.
+
image:vmware-vcf-asa-image08.png["Scegliere adattatore di rete VMkernel"]

+
{nbsp}

. Nella pagina *Seleziona dispositivo di destinazione*, scegliere uno dei gruppi di porte distribuite per iSCSI creati in precedenza.
+
image:vmware-vcf-asa-image95.png["Scegliere il gruppo di porte di destinazione"]

+
{nbsp}

. Nella pagina *Proprietà porta* fare clic sulla casella *NVMe su TCP* e fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-image96.png["Proprietà della porta VMkernel"]

+
{nbsp}

. Nella pagina *IPv4 settings* compilare i campi *IP address*, *Subnet mask* e fornire un nuovo indirizzo IP del gateway (solo se necessario). Fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-image97.png["Impostazioni di VMkernel IPv4"]

+
{nbsp}

. Rivedere le selezioni nella pagina *Pronto per il completamento* e fare clic su *fine* per creare l'adattatore VMkernel.
+
image:vmware-vcf-asa-image98.png["Esaminare le selezioni di VMkernel"]

+
{nbsp}

. Ripetere questa procedura per creare un adattatore VMkernel per la seconda rete iSCSI.


====
.Aggiungi adattatore NVMe over TCP
[%collapsible]
====
Ogni host ESXi nel cluster del dominio del carico di lavoro deve avere installato un adattatore software NVMe over TCP per ogni rete NVMe/TCP consolidata dedicata al traffico storage.

Per installare gli adattatori NVMe over TCP e rilevare i controller NVMe, attenersi alla seguente procedura:

. Nel client vSphere, accedere a uno degli host ESXi nel cluster del dominio del carico di lavoro. Dalla scheda *Configure* (Configura), fare clic su *Storage Adapters* (schede di memoria) nel menu a discesa *Add Software Adapter* (Aggiungi scheda software) e selezionare *Add NVMe over TCP adapter* (Aggiungi scheda NVMe su TCP).
+
image:vmware-vcf-asa-image99.png["Aggiungi adattatore NVMe over TCP"]

+
{nbsp}

. Nella finestra *Add Software NVMe over TCP adapter* (Aggiungi adattatore NVMe su TCP), accedere al menu a discesa *Physical Network Adapter* (scheda di rete fisica) e selezionare l'adattatore di rete fisico corretto su cui abilitare l'adattatore NVMe.
+
image:vmware-vcf-asa-image100.png["Selezionare l'adattatore fisico"]

+
{nbsp}

. Ripetere questa procedura per la seconda rete assegnata al traffico NVMe su TCP, assegnando l'adattatore fisico corretto.
. Selezionare una delle schede NVMe over TCP appena installate e, nella scheda *Controller*, selezionare *Aggiungi controller*.
+
image:vmware-vcf-asa-image101.png["Aggiungi controller"]

+
{nbsp}

. Nella finestra *Aggiungi controller*, selezionare la scheda *automaticamente* e completare i seguenti passaggi.
+
** Immettere gli indirizzi IP per una delle interfacce logiche SVM sulla stessa rete dell'adattatore fisico assegnato a questo adattatore NVMe over TCP.
** Fare clic sul pulsante *Scopri controller*.
** Dall'elenco dei controller rilevati, fare clic sulla casella di controllo per i due controller con indirizzi di rete allineati con questo adattatore NVMe over TCP.
** Fare clic sul pulsante *OK* per aggiungere i controller selezionati.
+
image:vmware-vcf-asa-image102.png["Rilevamento e aggiunta di controller"]

+
{nbsp}



. Dopo qualche secondo dovresti vedere il namespace NVMe nella scheda Devices (dispositivi).
+
image:vmware-vcf-asa-image103.png["Namespace NVMe elencato nei dispositivi"]

+
{nbsp}

. Ripetere questa procedura per creare un adattatore NVMe over TCP per la seconda rete stabilita per il traffico NVMe/TCP.


====
.Implementa datastore NVMe su TCP
[%collapsible]
====
Per creare un datastore VMFS nel namespace NVMe, completa i seguenti passaggi:

. Nel client vSphere, accedere a uno degli host ESXi nel cluster del dominio del carico di lavoro. Dal menu *azioni*, selezionare *archiviazione > nuovo archivio dati...*.
+
image:vmware-vcf-asa-image104.png["Aggiungi adattatore NVMe over TCP"]

+
{nbsp}

. Nella procedura guidata *nuovo datastore*, selezionare *VMFS* come tipo. Fare clic su *Avanti* per continuare.
. Nella pagina *selezione nome e dispositivo*, fornire un nome per l'archivio dati e selezionare lo spazio dei nomi NVMe dall'elenco dei dispositivi disponibili.
+
image:vmware-vcf-asa-image105.png["Selezione del nome e del dispositivo"]

+
{nbsp}

. Nella pagina *VMFS versione* selezionare la versione di VMFS per il datastore.
. Nella pagina *Partition Configuration*, apportare le modifiche desiderate allo schema di partizione predefinito. Fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-image106.png["Configurazione delle partizioni NVMe"]

+
{nbsp}

. Nella pagina *Pronto per il completamento*, rivedere il riepilogo e fare clic su *fine* per creare il datastore.
. Accedere al nuovo datastore nell'inventario e fare clic sulla scheda *hosts*. Se configurato correttamente, tutti gli host ESXi nel cluster devono essere elencati e avere accesso al nuovo datastore.
+
image:vmware-vcf-asa-image107.png["Host connessi al datastore"]

+
{nbsp}



====


== Ulteriori informazioni

Per informazioni sulla configurazione dei sistemi storage ONTAP, consultare la link:https://docs.netapp.com/us-en/ontap["Documentazione di ONTAP 9"] centro.

Per informazioni sulla configurazione di VCF, fare riferimento a. link:https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html["Documentazione di VMware Cloud Foundation"].
