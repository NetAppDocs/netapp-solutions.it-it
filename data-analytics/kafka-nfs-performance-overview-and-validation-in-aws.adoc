---
sidebar: sidebar 
permalink: data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html 
keywords: AWS cloud, ha pair, high availability, openmessage benchmarking, architectural setup 
summary: Un cluster Kafka con il layer di storage montato su NetApp NFS è stato sottoposto a benchmark per le performance nel cloud AWS. Gli esempi di benchmarking sono descritti nelle sezioni seguenti. 
---
= Panoramica e convalida delle performance in AWS - NetApp Cloud Volume ONTAP
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:kafka-nfs-why-netapp-nfs-for-kafka-workloads.html["Precedente: Perché scegliere NetApp NFS per i workload Kafka?."]

[role="lead"]
Un cluster Kafka con il layer di storage montato su NetApp NFS è stato sottoposto a benchmark per le performance nel cloud AWS. Gli esempi di benchmarking sono descritti nelle sezioni seguenti.



== Kafka nel cloud AWS con NetApp Cloud Volumes ONTAP (coppia ad alta disponibilità e nodo singolo)

Un cluster Kafka con NetApp Cloud Volumes ONTAP (coppia ha) è stato sottoposto a benchmark per le performance nel cloud AWS. Questo benchmarking è descritto nelle sezioni seguenti.



=== Configurazione architetturale

La seguente tabella mostra la configurazione ambientale per un cluster Kafka che utilizza NAS.

|===
| Componente della piattaforma | Configurazione dell'ambiente 


| Kafka 3.2.3  a| 
* 3 zookeeper – t2.small
* 3 server di broker – i3en.2xlarge
* 1 x Grafana – c5n.2xlarge
* 4 x produttore/consumatore -- c5n.2xlargo *




| Sistema operativo su tutti i nodi | RHEL8.6 


| Istanza di NetApp Cloud Volumes ONTAP | Istanza coppia HA – m5dn.12xLarge x istanza nodo singolo 2node - m5dn.12xLarge x 1 nodo 
|===


=== Configurazione di NetApp Cluster Volume ONTAP

. Per la coppia Cloud Volumes ONTAP ha, abbiamo creato due aggregati con tre volumi su ciascun aggregato su ciascun controller di storage. Per il singolo nodo Cloud Volumes ONTAP, creiamo sei volumi in un aggregato.
+
image:kafka-nfs-image25.png["Questa immagine mostra le proprietà di aggr3 e aggr22."]

+
image:kafka-nfs-image26.png["Questa immagine mostra le proprietà di aggr2."]

. Per ottenere performance di rete migliori, abbiamo attivato il networking ad alta velocità sia per la coppia ha che per il singolo nodo.
+
image:kafka-nfs-image27.png["Queste immagini mostrano come abilitare il networking ad alta velocità."]

. Abbiamo notato che la NVRAM ONTAP aveva più IOPS, quindi abbiamo modificato gli IOPS a 2350 per il volume root Cloud Volumes ONTAP. Il disco del volume root in Cloud Volumes ONTAP aveva una dimensione di 47 GB. Il seguente comando ONTAP è per la coppia ha e lo stesso passo è applicabile per il singolo nodo.
+
....
statistics start -object vnvram -instance vnvram -counter backing_store_iops -sample-id sample_555
kafka_nfs_cvo_ha1::*> statistics show -sample-id sample_555
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-01
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1479
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-02
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1210
2 entries were displayed.
kafka_nfs_cvo_ha1::*>
....
+
image:kafka-nfs-image28.png["Queste immagini mostrano come modificare le proprietà del volume."]



La figura seguente mostra l'architettura di un cluster Kafka basato su NAS.

* *Compute.* abbiamo utilizzato un cluster Kafka a tre nodi con un ensemble di zookeeper a tre nodi in esecuzione su server dedicati. Ciascun broker disponeva di due punti di montaggio NFS su un singolo volume nell'istanza di Cloud Volumes ONTAP tramite un LIF dedicato.
* *Monitoring.* abbiamo utilizzato due nodi per una combinazione Prometheus-Grafana. Per la generazione dei carichi di lavoro, abbiamo utilizzato un cluster a tre nodi separato in grado di produrre e utilizzare questo cluster Kafka.
* *Storage.* abbiamo utilizzato un'istanza di ha-Pair Cloud Volumes ONTAP con un volume AWS-EBS GP3 da 6 TB montato sull'istanza. Il volume è stato quindi esportato nel broker Kafka con un montaggio NFS.


image:kafka-nfs-image29.png["Questa figura illustra l'architettura di un cluster Kafka basato su NAS."]



=== Configurazioni di benchmarking di OpenMessage

. Per migliorare le performance NFS, abbiamo bisogno di più connessioni di rete tra il server NFS e il client NFS, che possono essere create utilizzando nconnect. Montare i volumi NFS sui nodi di broker con l'opzione nconnect eseguendo il seguente comando:
+
....
[root@ip-172-30-0-121 ~]# cat /etc/fstab
UUID=eaa1f38e-de0f-4ed5-a5b5-2fa9db43bb38/xfsdefaults00
/dev/nvme1n1 /mnt/data-1 xfs defaults,noatime,nodiscard 0 0
/dev/nvme2n1 /mnt/data-2 xfs defaults,noatime,nodiscard 0 0
172.30.0.233:/kafka_aggr3_vol1 /kafka_aggr3_vol1 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol2 /kafka_aggr3_vol2 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol3 /kafka_aggr3_vol3 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol1 /kafka_aggr22_vol1 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol2 /kafka_aggr22_vol2 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol3 /kafka_aggr22_vol3 nfs defaults,nconnect=16 0 0
[root@ip-172-30-0-121 ~]# mount -a
[root@ip-172-30-0-121 ~]# df -h
Filesystem                       Size  Used Avail Use% Mounted on
devtmpfs                          31G     0   31G   0% /dev
tmpfs                             31G  249M   31G   1% /run
tmpfs                             31G     0   31G   0% /sys/fs/cgroup
/dev/nvme0n1p2                    10G  2.8G  7.2G  28% /
/dev/nvme1n1                     2.3T  248G  2.1T  11% /mnt/data-1
/dev/nvme2n1                     2.3T  245G  2.1T  11% /mnt/data-2
172.30.0.233:/kafka_aggr3_vol1   1.0T   12G 1013G   2% /kafka_aggr3_vol1
172.30.0.233:/kafka_aggr3_vol2   1.0T  5.5G 1019G   1% /kafka_aggr3_vol2
172.30.0.233:/kafka_aggr3_vol3   1.0T  8.9G 1016G   1% /kafka_aggr3_vol3
172.30.0.242:/kafka_aggr22_vol1  1.0T  7.3G 1017G   1% /kafka_aggr22_vol1
172.30.0.242:/kafka_aggr22_vol2  1.0T  6.9G 1018G   1% /kafka_aggr22_vol2
172.30.0.242:/kafka_aggr22_vol3  1.0T  5.9G 1019G   1% /kafka_aggr22_vol3
tmpfs                            6.2G     0  6.2G   0% /run/user/1000
[root@ip-172-30-0-121 ~]#
....
. Verificare le connessioni di rete in Cloud Volumes ONTAP. Il seguente comando ONTAP viene utilizzato dal singolo nodo Cloud Volumes ONTAP. Lo stesso passaggio si applica alla coppia Cloud Volumes ONTAP ha.
+
....
Last login time: 1/20/2023 00:16:29
kafka_nfs_cvo_sn::> network connections active show -service nfs* -fields remote-host
node                cid        vserver              remote-host
------------------- ---------- -------------------- ------------
kafka_nfs_cvo_sn-01 2315762628 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762629 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762630 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762631 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762632 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762633 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762634 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762635 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762636 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762637 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762639 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762640 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762641 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762642 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762643 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762644 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762645 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762646 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762647 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762648 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762649 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762650 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762651 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762652 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762653 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762656 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762657 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762658 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762659 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762660 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762661 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762662 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762663 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762664 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762665 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762666 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762667 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762668 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762669 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762670 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762671 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762672 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762673 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762674 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762676 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762677 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762678 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762679 svm_kafka_nfs_cvo_sn 172.30.0.223
48 entries were displayed.
 
kafka_nfs_cvo_sn::>
....
. Utilizziamo il seguente Kafka `server.properties` In tutti i broker Kafka per la coppia Cloud Volumes ONTAP ha. Il `log.dirs` la proprietà è diversa per ogni broker e le proprietà rimanenti sono comuni per gli broker. Per il broker1, il `log.dirs` il valore è il seguente:
+
....
[root@ip-172-30-0-121 ~]# cat /opt/kafka/config/server.properties
broker.id=0
advertised.listeners=PLAINTEXT://172.30.0.121:9092
#log.dirs=/mnt/data-1/d1,/mnt/data-1/d2,/mnt/data-1/d3,/mnt/data-2/d1,/mnt/data-2/d2,/mnt/data-2/d3
log.dirs=/kafka_aggr3_vol1/broker1,/kafka_aggr3_vol2/broker1,/kafka_aggr3_vol3/broker1,/kafka_aggr22_vol1/broker1,/kafka_aggr22_vol2/broker1,/kafka_aggr22_vol3/broker1
zookeeper.connect=172.30.0.12:2181,172.30.0.30:2181,172.30.0.178:2181
num.network.threads=64
num.io.threads=64
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
replica.fetch.max.bytes=524288000
background.threads=20
num.replica.alter.log.dirs.threads=40
num.replica.fetchers=20
[root@ip-172-30-0-121 ~]#
....
+
** Per il broker2, il `log.dirs` il valore della proprietà è il seguente:
+
....
log.dirs=/kafka_aggr3_vol1/broker2,/kafka_aggr3_vol2/broker2,/kafka_aggr3_vol3/broker2,/kafka_aggr22_vol1/broker2,/kafka_aggr22_vol2/broker2,/kafka_aggr22_vol3/broker2
....
** Per il broker3, il `log.dirs` il valore della proprietà è il seguente:
+
....
log.dirs=/kafka_aggr3_vol1/broker3,/kafka_aggr3_vol2/broker3,/kafka_aggr3_vol3/broker3,/kafka_aggr22_vol1/broker3,/kafka_aggr22_vol2/broker3,/kafka_aggr22_vol3/broker3
....


. Per il singolo nodo Cloud Volumes ONTAP, Kafka `servers.properties` È uguale alla coppia Cloud Volumes ONTAP ha, ad eccezione di `log.dirs` proprietà.
+
** Per il broker1, il `log.dirs` il valore è il seguente:
+
....
log.dirs=/kafka_aggr2_vol1/broker1,/kafka_aggr2_vol2/broker1,/kafka_aggr2_vol3/broker1,/kafka_aggr2_vol4/broker1,/kafka_aggr2_vol5/broker1,/kafka_aggr2_vol6/broker1
....
** Per il broker2, il `log.dirs` il valore è il seguente:
+
....
log.dirs=/kafka_aggr2_vol1/broker2,/kafka_aggr2_vol2/broker2,/kafka_aggr2_vol3/broker2,/kafka_aggr2_vol4/broker2,/kafka_aggr2_vol5/broker2,/kafka_aggr2_vol6/broker2
....
** Per il broker3, il `log.dirs` il valore della proprietà è il seguente:
+
....
log.dirs=/kafka_aggr2_vol1/broker3,/kafka_aggr2_vol2/broker3,/kafka_aggr2_vol3/broker3,/kafka_aggr2_vol4/broker3,/kafka_aggr2_vol5/broker3,/kafka_aggr2_vol6/broker3
....


. Il carico di lavoro nell'OMB viene configurato con le seguenti proprietà: `(/opt/benchmark/workloads/1-topic-100-partitions-1kb.yaml)`.
+
....
topics: 4
partitionsPerTopic: 100
messageSize: 32768
useRandomizedPayloads: true
randomBytesRatio: 0.5
randomizedPayloadPoolSize: 100
subscriptionsPerTopic: 1
consumerPerSubscription: 80
producersPerTopic: 40
producerRate: 1000000
consumerBacklogSizeGB: 0
testDurationMinutes: 5
....
+
Il `messageSize` può variare in base al caso di utilizzo. Nel nostro test delle performance, abbiamo utilizzato 3K.

+
Abbiamo utilizzato due diversi driver, Sync o throughput, da OMB per generare il carico di lavoro sul cluster Kafka.

+
** Il file yaml utilizzato per le proprietà del driver Sync è il seguente `(/opt/benchmark/driver- kafka/kafka-sync.yaml)`:
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
  flush.messages=1
  flush.ms=0
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....
** Il file yaml utilizzato per le proprietà del driver di throughput è il seguente `(/opt/benchmark/driver- kafka/kafka-throughput.yaml)`:
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
  default.api.timeout.ms=1200000
  request.timeout.ms=1200000
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....






== Metodologia di test

. È stato eseguito il provisioning di un cluster Kafka secondo le specifiche descritte in precedenza utilizzando Terraform e Ansible. Il terraform viene utilizzato per costruire l'infrastruttura utilizzando istanze AWS per il cluster Kafka e Ansible crea il cluster Kafka su di essi.
. È stato attivato un carico di lavoro OMB con la configurazione del carico di lavoro descritta sopra e il driver Sync.
+
....
Sudo bin/benchmark –drivers driver-kafka/kafka- sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. È stato attivato un altro carico di lavoro con il driver di throughput con la stessa configurazione del carico di lavoro.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




== Osservazione

Sono stati utilizzati due diversi tipi di driver per generare carichi di lavoro per confrontare le performance di un'istanza di Kafka in esecuzione su NFS. La differenza tra i driver è la proprietà di scaricamento dei log.

Per una coppia Cloud Volumes ONTAP ha:

* Throughput totale generato in modo coerente dal driver Sync: ~1236 Mbps.
* Throughput totale generato per il driver di throughput: Picco ~1412 Mbps.


Per un singolo nodo Cloud Volumes ONTAP:

* Throughput totale generato in modo coerente dal driver Sync: ~ 1962 MBps.
* Throughput totale generato dal driver di throughput: Picco ~1660 MBps


Il driver Sync è in grado di generare un throughput coerente quando i log vengono trasferiti istantaneamente sul disco, mentre il driver di throughput genera burst di throughput quando i log vengono impegnati su disco in massa.

Questi numeri di throughput vengono generati per la configurazione AWS specificata. Per requisiti di performance più elevati, i tipi di istanze possono essere scalati e ottimizzati ulteriormente per ottenere numeri di throughput migliori. Il throughput totale o il tasso totale è la combinazione di un tasso di produttore e di consumo.

image:kafka-nfs-image30.png["Qui sono presentati quattro grafici diversi. Driver di throughput coppia CVO-ha. Driver CVO-ha Pair Sync. Driver di throughput CVO a nodo singolo. Driver CVO-single node Sync."]

Verificare il throughput dello storage durante l'esecuzione del benchmarking del throughput o del driver di sincronizzazione.

image:kafka-nfs-image31.png["Questo grafico mostra le performance in termini di latenza, IOPS e throughput."]



== Apache Kafka in AWS FSX per NetApp ONTAP



=== Panoramica

Network file System (NFS) è un file system di rete ampiamente utilizzato per la memorizzazione di grandi quantità di dati. Nella maggior parte delle organizzazioni i dati vengono sempre più generati da applicazioni di streaming come Apache Kafka. Questi carichi di lavoro richiedono scalabilità, bassa latenza e una solida architettura di acquisizione dei dati con moderne funzionalità di storage. Per consentire l'analisi in tempo reale e fornire informazioni utili, è necessaria un'infrastruttura ben progettata e dalle performance elevate.

Kafka di progettazione funziona con file system compatibile con POSIX e si affida al file system per gestire le operazioni sui file, ma quando si memorizzano i dati su un file system NFSv3, il client NFS del broker Kafka può interpretare le operazioni sui file in modo diverso da un file system locale come XFS o Ext4. Un esempio comune è il ridenominazione di NFS Silly, che ha causato il fallimento dei broker Kafka durante l'espansione dei cluster e la riallocazione delle partizioni. Per far fronte a questa sfida, NetApp ha aggiornato il client NFS open-source Linux con le modifiche ora generalmente disponibili in RHEL8.7, RHEL9.1 e supportate dall'attuale versione di FSX per NetApp ONTAP, ONTAP 9.12.1.

Amazon FSX per NetApp ONTAP offre un file system NFS completamente gestito, scalabile e dalle performance elevate nel cloud. I dati Kafka su FSX per NetApp ONTAP possono scalare per gestire grandi quantità di dati e garantire la tolleranza agli errori. NFS offre gestione dello storage centralizzata e protezione dei dati per set di dati critici e sensibili.

Questi miglioramenti consentono ai clienti AWS di sfruttare FSX per NetApp ONTAP quando eseguono carichi di lavoro Kafka su servizi di calcolo AWS. Questi vantaggi sono:
* Riduzione dell'utilizzo della CPU per ridurre i tempi di attesa i/O.
* Tempi di recovery più rapidi per i broker Kafka.
* Affidabilità ed efficienza.
* Scalabilità e performance.
* Disponibilità multi-Availability zone.
* Protezione dei dati.



=== Panoramica e convalida delle performance in AWS FSX per NetApp ONTAP

Un cluster Kafka con il layer di storage montato su NetApp NFS è stato sottoposto a benchmark per le performance nel cloud AWS. Gli esempi di benchmarking sono descritti nelle sezioni seguenti.



==== Kafka in AWS FSX per NetApp ONTAP

Un cluster Kafka con AWS FSX per NetApp ONTAP è stato sottoposto a benchmark per le performance nel cloud AWS. Questo benchmarking è descritto nelle sezioni seguenti.



==== Configurazione architetturale

La seguente tabella mostra la configurazione ambientale per un cluster Kafka che utilizza AWS FSX per NetApp ONTAP.

|===
| Componente della piattaforma | Configurazione dell'ambiente 


| Kafka 3.2.3  a| 
* 3 zookeeper – t2.small
* 3 server di broker – i3en.2xlarge
* 1 x Grafana – c5n.2xlarge
* 4 x produttore/consumatore -- c5n.2xlargo *




| Sistema operativo su tutti i nodi | RHEL8.6 


| AWS FSX per NetApp ONTAP | Multi-AZ con throughput di 4 GB/sec e 160000 IOPS 
|===


==== Configurazione di NetApp FSX per NetApp ONTAP

. Per il test iniziale, abbiamo creato un file system FSX per NetApp ONTAP con 2 TB di capacità e 40000 IOPS per un throughput di 2 GB/sec.
+
....
[root@ip-172-31-33-69 ~]# aws fsx create-file-system --region us-east-2  --storage-capacity 2048 --subnet-ids <desired subnet 1> subnet-<desired subnet 2> --file-system-type ONTAP --ontap-configuration DeploymentType=MULTI_AZ_HA_1,ThroughputCapacity=2048,PreferredSubnetId=<desired primary subnet>,FsxAdminPassword=<new password>,DiskIopsConfiguration="{Mode=USER_PROVISIONED,Iops=40000"}
....
+
Nel nostro esempio, stiamo implementando FSX per NetApp ONTAP attraverso l'interfaccia CLI AWS. Sarà necessario personalizzare ulteriormente il comando nell'ambiente in base alle esigenze. FSX per NetApp ONTAP può inoltre essere implementato e gestito tramite la console AWS per un'esperienza di implementazione più semplice e ottimizzata con meno input dalla riga di comando.

+
Documentazione in FSX per NetApp ONTAP, il numero massimo di IOPS ottenibili per un file system con throughput di 2 GB/sec nella nostra area di test (US-Est-1) è 80,000 iops. Il totale massimo di iops per un file system FSX per NetApp ONTAP è di 160,000 iops, che richiede un'implementazione di throughput di 4 GB/sec per ottenere il risultato che verrà dimostrato più avanti in questo documento.

+
Per ulteriori informazioni sulle specifiche delle prestazioni di FSX per NetApp ONTAP, visita la documentazione di AWS FSX per NetApp ONTAP qui: https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/performance.html[] .

+
La sintassi dettagliata della riga di comando per FSX "create-file-system" è disponibile qui: https://docs.aws.amazon.com/cli/latest/reference/fsx/create-file-system.html[]

+
Ad esempio, è possibile specificare una chiave KMS specifica invece della chiave master AWS FSX predefinita utilizzata quando non viene specificata alcuna chiave KMS.

. Durante la creazione del file system FSX per NetApp ONTAP, attendere che lo stato "Lifecycle" (ciclo di vita) passi a "AVAILABLE" (DISPONIBILE) nel ritorno JSON dopo aver descritto il file system come segue:
+
....
[root@ip-172-31-33-69 ~]# aws fsx describe-file-systems  --region us-east-1 --file-system-ids fs-02ff04bab5ce01c7c
....
. Convalidare le credenziali effettuando l'accesso a FSX per SSH NetApp ONTAP con l'utente fsxadmin:
Fsxadmin è l'account admin predefinito per FSX per i filesystem NetApp ONTAP al momento della creazione. La password per fsxadmin è la password che è stata configurata durante la prima creazione del file system nella console AWS o con l'interfaccia CLI AWS, come è stato completato nella fase 1.
+
....
[root@ip-172-31-33-69 ~]# ssh fsxadmin@198.19.250.244
The authenticity of host '198.19.250.244 (198.19.250.244)' can't be established.
ED25519 key fingerprint is SHA256:mgCyRXJfWRc2d/jOjFbMBsUcYOWjxoIky0ltHvVDL/Y.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '198.19.250.244' (ED25519) to the list of known hosts.
(fsxadmin@198.19.250.244) Password:

This is your first recorded login.
....
. Una volta convalidate le credenziali, creare la macchina virtuale di storage sul file system FSX per NetApp ONTAP
+
....
[root@ip-172-31-33-69 ~]# aws fsx --region us-east-1 create-storage-virtual-machine --name svmkafkatest --file-system-id fs-02ff04bab5ce01c7c
....
+
Una macchina virtuale per lo storage (SVM) è un file server isolato con le proprie credenziali amministrative ed endpoint per l'amministrazione e l'accesso ai dati in FSX per i volumi NetApp ONTAP e fornisce FSX per il multi-tenancy NetApp ONTAP.

. Una volta configurata la macchina virtuale di storage primaria, SSH nel nuovo file system FSX per NetApp ONTAP e creare volumi nella macchina virtuale di storage utilizzando il comando di esempio riportato di seguito. Analogamente, creiamo 6 volumi per questa convalida. In base alla nostra convalida, mantenere il costituente predefinito (8) o un numero inferiore di costituenti che forniranno prestazioni migliori a kafka.
+
....
FsxId02ff04bab5ce01c7c::*> volume create -volume kafkafsxN1 -state online -policy default -unix-permissions ---rwxr-xr-x -junction-active true -type RW -snapshot-policy none  -junction-path /kafkafsxN1 -aggr-list aggr1
....
. Per i nostri test, abbiamo bisogno di capacità aggiuntiva nei nostri volumi. Estendere le dimensioni del volume a 2 TB e montarlo sul percorso di giunzione.
+
....
FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN1 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN1" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN2 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN2" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN3 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN3" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN4 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN4" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN5 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN5" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN6 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN6" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume show -vserver svmkafkatest -volume *
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
svmkafkatest
          kafkafsxN1   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN2   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN3   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN4   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN5   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN6   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          svmkafkatest_root
                       aggr1        online     RW          1GB    968.1MB    0%
7 entries were displayed.

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN1 -junction-path /kafkafsxN1

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN2 -junction-path /kafkafsxN2

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN3 -junction-path /kafkafsxN3

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN4 -junction-path /kafkafsxN4

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN5 -junction-path /kafkafsxN5

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN6 -junction-path /kafkafsxN6
....
+
In FSX per NetApp ONTAP, è possibile eseguire il thin provisioning dei volumi. Nel nostro esempio, la capacità totale del volume esteso supera la capacità totale del file system, quindi sarà necessario estendere la capacità totale del file system per sbloccare la capacità aggiuntiva del volume sottoposto a provisioning, come illustrato nella fase successiva.

. Inoltre, per ottenere maggiori performance e capacità, estendiamo la capacità di throughput FSX per NetApp ONTAP da 2 GB/sec a 4 GB/sec e IOPS a 160000 e la capacità a 5 TB
+
....
[root@ip-172-31-33-69 ~]# aws fsx update-file-system --region us-east-1  --storage-capacity 5120 --ontap-configuration 'ThroughputCapacity=4096,DiskIopsConfiguration={Mode=USER_PROVISIONED,Iops=160000}' --file-system-id fs-02ff04bab5ce01c7c
....
+
La sintassi dettagliata della riga di comando per FSX "update-file-system" è disponibile qui:
https://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html[]

. I volumi FSX per NetApp ONTAP sono montati con opzioni nconnect e predefinite nei broker Kafka
+
La seguente immagine mostra l'architettura finale di un cluster Kafka basato su FSX per NetApp ONTAP:

+
image:aws-fsx-kafka-arch1.png["Questa immagine mostra l'architettura di un cluster Kafka basato su FSxN."]

+
** Calcolo. Abbiamo utilizzato un cluster Kafka a tre nodi con un gruppo di zookeeper a tre nodi in esecuzione su server dedicati. Ciascun broker disponeva di sei punti di montaggio NFS su sei volumi nell'istanza FSX per NetApp ONTAP.
** Monitoraggio. Abbiamo utilizzato due nodi per una combinazione Prometheus-Grafana. Per la generazione dei carichi di lavoro, abbiamo utilizzato un cluster a tre nodi separato in grado di produrre e utilizzare questo cluster Kafka.
** Storage. Abbiamo utilizzato un FSX per NetApp ONTAP con sei volumi da 2 TB montati. Il volume è stato quindi esportato nel broker Kafka con un montaggio NFS. I volumi FSX per NetApp ONTAP sono montati con 16 sessioni Nconnect e opzioni predefinite nei broker Kafka.






==== Configurazioni di benchmarking di OpenMessage.

Abbiamo utilizzato la stessa configurazione utilizzata per NetApp Cloud Volumes ONTAP e i relativi dettagli sono qui -
https://docs.netapp.com/us-en/netapp-solutions/data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html#architectural-setup[]



==== Metodologia di test

. È stato eseguito il provisioning di un cluster Kafka in base alle specifiche descritte in precedenza utilizzando Terraform e ansible. Il terraform viene utilizzato per costruire l'infrastruttura utilizzando istanze AWS per il cluster Kafka e ansible crea il cluster Kafka su di essi.
. È stato attivato un carico di lavoro OMB con la configurazione del carico di lavoro descritta sopra e il driver Sync.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. È stato attivato un altro carico di lavoro con il driver di throughput con la stessa configurazione del carico di lavoro.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




==== Osservazione

Sono stati utilizzati due diversi tipi di driver per generare carichi di lavoro per confrontare le performance di un'istanza di Kafka in esecuzione su NFS. La differenza tra i driver è la proprietà di scaricamento dei log.

Per un fattore di replica Kafka 1 e FSX per NetApp ONTAP:

* Throughput totale generato in modo coerente dal driver Sync: ~ 3218 Mbps e performance di picco in ~ 3652 Mbps.
* Throughput totale generato in modo coerente dal driver di throughput: ~ 3679 Mbps e performance di picco in ~ 3908 Mbps.


Per Kafka con fattore di replica 3 e FSX per NetApp ONTAP :

* Throughput totale generato in modo coerente dal driver Sync: ~ 1252 Mbps e performance di picco in ~ 1382 Mbps.
* Throughput totale generato in modo coerente dal driver di throughput: ~ 1218 Mbps e performance di picco in ~ 1328 Mbps.


Nel fattore 3 di replica di Kafka, l'operazione di lettura e scrittura è stata eseguita tre volte su FSX per NetApp ONTAP, nel fattore 1 di replica di Kafka, l'operazione di lettura e scrittura è una volta su FSX per NetApp ONTAP, quindi in entrambe le procedure di convalida, Siamo in grado di raggiungere il throughput massimo di 4 GB/sec.

Il driver Sync è in grado di generare un throughput coerente quando i log vengono trasferiti istantaneamente sul disco, mentre il driver di throughput genera burst di throughput quando i log vengono impegnati su disco in massa.

Questi numeri di throughput vengono generati per la configurazione AWS specificata. Per requisiti di performance più elevati, i tipi di istanze possono essere scalati e ottimizzati ulteriormente per ottenere numeri di throughput migliori. Il throughput totale o il tasso totale è la combinazione di un tasso di produttore e di consumo.

image:aws-fsxn-performance-rf-1-rf-3.png["Questa immagine mostra le performance di kafka con RF1 e RF3"]

Il grafico riportato di seguito mostra le prestazioni FSX da 2 GB/sec per NetApp ONTAP e da 4 GB/sec per il fattore di replica Kafka 3. Il fattore di replica 3 esegue tre volte l'operazione di lettura e scrittura su FSX per lo storage NetApp ONTAP. La velocità totale per il driver di throughput è di 881 MB/sec, che esegue operazioni di lettura e scrittura Kafka di circa 2.64 GB/sec sul file system FSX da 2 GB/sec per NetApp ONTAP, mentre la velocità totale per il driver di throughput è di 1328 MB/sec che esegue operazioni di lettura e scrittura kafka di circa 3.98 GB/sec. Le performance di Kafka sono lineari e scalabili in base al throughput FSX per NetApp ONTAP.

image:aws-fsxn-2gb-4gb-scale.png["Questa immagine mostra le performance scale-out di 2 GB/sec e 4 GB/sec."]

Il grafico seguente mostra le performance tra l'istanza EC2 e FSX per NetApp ONTAP (fattore di replica Kafka: 3)

image:aws-fsxn-ec2-fsxn-comparition.png["Questa immagine mostra il confronto delle performance di EC2 rispetto a FSxN in RF3."]

link:kafka-nfs-performance-overview-and-validation-with-aff-on-premises.html["Pagina successiva: Panoramica delle performance e validazione con AFF on-premise."]
