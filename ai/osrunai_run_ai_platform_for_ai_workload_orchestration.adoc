---
sidebar: sidebar 
permalink: ai/osrunai_run_ai_platform_for_ai_workload_orchestration.html 
keywords:  
summary:  
---
= Run:ai Platform for ai workload Orchestration
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
* Tempi di innovazione più rapidi. Utilizzando i meccanismi di pooling delle risorse Run:ai, accodamento e prioritizzazione insieme al sistema storage NetApp, i ricercatori vengono rimossi dai problemi di gestione dell'infrastruttura e possono concentrarsi esclusivamente sulla scienza dei dati. Esegui: I clienti ai e NetApp aumentano la produttività eseguendo tutti i carichi di lavoro necessari senza colli di bottiglia della pipeline di dati o di calcolo.
* Aumento della produttività del team. Gli algoritmi Run:ai Fairness garantiscono che tutti gli utenti e i team ottenano la loro giusta quota di risorse. È possibile preimpostare le policy relative ai progetti prioritari e la piattaforma consente l'allocazione dinamica delle risorse da un team di utenti all'altro, consentendo agli utenti di ottenere un accesso tempestivo alle risorse GPU più ambite.
* Utilizzo della GPU migliorato. Run:ai Scheduler consente agli utenti di utilizzare facilmente GPU frazionali, GPU interi e nodi multipli di GPU per la formazione distribuita su Kubernetes. In questo modo, i carichi di lavoro ai vengono eseguiti in base alle esigenze, non alla capacità. Team di data science in grado di eseguire più esperimenti di ai sulla stessa infrastruttura.

