---
sidebar: sidebar 
permalink: ai/rag_concepts_components.html 
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP 
summary: RAG aziendale con NetApp - concetti e componenti 
---
= Concetti e componenti
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/




== Ai generativa

I sistemi ai come l'ai generativa sono progettati applicando l'apprendimento automatico non supervisionato o supervisionato a un ampio set di dati. A differenza dei tradizionali modelli di machine learning che fanno previsioni su uno specifico set di dati, i modelli ai generativi sono in grado di generare nuovi contenuti come testo, codice, immagine, video o audio, in risposta alle richieste dell'utente. Per questo motivo, le capacità di un sistema ai generativo sono classificate anche in base alla modalità o al tipo di dati utilizzati. Questi potrebbero essere unimodali o multimodali. Un sistema unimodale prende solo un tipo di ingresso (es. Solo testo o solo immagine), mentre un sistema multimodale può assumere più di un tipo di input (es. testo, immagine e audio), capiscono e generano contemporaneamente contenuti in diverse modalità. In sostanza, l'intelligenza artificiale generativa sta cambiando il modo in cui le aziende creano contenuti, generano nuovi concetti di progettazione ed estraggono valore dai dati esistenti.



=== Modelli di lingue grandi (LLMS)

Le LLMS sono modelli di apprendimento approfondito preformati su grandi quantità di dati, che possono riconoscere e generare testo, tra le altre attività. LLMS ha iniziato come un sottoinsieme di IA generativa che si è focalizzato principalmente sul linguaggio, tuttavia, tali distinzioni stanno lentamente sbiadendo come LLMS multimodale continuano ad emergere. Il trasformatore sottostante in un LLM, introduce una nuova architettura di rete diversa da RNN o CNN. Ha un insieme di reti neurali che consistono di un codificatore e di un decodificatore che aiuta ad estrarre il significato da una sequenza di testo e capire la relazione tra le parole. LLMS è in grado di rispondere al linguaggio umano naturale e utilizzare l'analisi dei dati per rispondere a una domanda non strutturata. Tuttavia, il sistema LLMS può essere affidabile solo quanto i dati che ingeriscono, quindi soggetto alle allucinazioni, derivanti dalle sfide dell'immondizia. Se i sistemi LLMS ricevono informazioni false, possono generare risultati imprecisi in risposta a query degli utenti semplicemente per adattarsi alla narrativa che stanno creando. La nostra ricerca basata su prove suggerisce che gli ingegneri dell'ai si affidano a vari metodi per contrastare queste allucinazioni, uno attraverso barriere che limitano risultati imprecisi e l'altro mettendo a punto e trasferendo l'apprendimento con dati di qualità contestualmente rilevanti, tramite tecniche come RAG.



=== Generazione aumentata di recupero (RAG)

I sistemi LLMS sono addestrati su grandi volumi di dati ma non sono addestrati sui vostri dati. RAG risolve questo problema aggiungendo i dati ai dati a cui LLMS ha già accesso. RAG consente ai clienti di sfruttare la potenza di un LLM, addestrato sui propri dati, recuperando così le informazioni e utilizzandole per fornire informazioni contestuali agli utenti ai generativi. RAG è una tecnica di machine learning, un approccio architetturale che può contribuire a ridurre le allucinazioni e migliorare l'efficacia e l'affidabilità del sistema LLMS, accelerare lo sviluppo di applicazioni ai e aumentare l'esperienza di ricerca aziendale.



=== Ragas

Esistono già strumenti e framework che consentono di costruire pipeline RAG, ma la loro valutazione e quantificazione delle performance della pipeline possono essere difficili. È qui che entra in gioco Ragas (valutazione RAG). Ragas è un framework che aiuta a valutare le pipeline RAG. Ragas mira a creare uno standard aperto, fornendo agli sviluppatori gli strumenti e le tecniche per sfruttare l'apprendimento continuo nelle loro applicazioni RAG. Per ulteriori informazioni, fare riferimento a. https://docs.ragas.io/en/stable/getstarted/index.html["Inizia a usare Ragas"^]



== Llama 3

Il modello Llama 3 di meta, un modello di trasformatore solo decodificatore, è un modello LLM (Large Language Model) preaddestrato e accessibile apertamente. Addestrato su oltre 15 trilioni di gettoni di dati, Llama 3 è una rivoluzione nella comprensione del linguaggio naturale (NLU). Eccelle nella comprensione contestuale e in attività complesse come la traduzione e la generazione del dialogo. Llama 3 è disponibile in due dimensioni: 8b TB per implementazione e sviluppo efficienti e 70B TB per applicazioni native ai su larga scala. I clienti possono implementare Llama 3 su Google Cloud tramite Vertex ai, in Azure tramite Azure ai Studio e su AWS tramite Amazon Sagemaker.

Nella nostra convalida abbiamo implementato il modello Llama di Meta con microservizi NVIDIA NEMO™, in un'istanza di calcolo NVIDIA DGX accelerata con GPU NVIDIA A100, per personalizzare e valutare un caso d'utilizzo ai generativo, supportando al tempo stesso la generazione aumentata del recupero (RAG) nelle applicazioni on-premise.



== Framework Open Source

Le seguenti informazioni aggiuntive sulle tecnologie open-source potrebbero essere rilevanti a seconda della distribuzione.



=== LangChain

LangChain è un framework di integrazione open-source per lo sviluppo di applicazioni basate su modelli di linguaggio di grandi dimensioni (LLMS). I clienti possono creare in modo efficiente applicazioni RAG in quanto vengono fornite con Document Loader, VectorStores e vari altri pacchetti, consentendo agli sviluppatori la flessibilità di creare flussi di lavoro complessi. Con LangSmith è inoltre possibile esaminare, monitorare e valutare le app per ottimizzare e distribuire costantemente qualsiasi catena in un'API REST con LangServe. LangChain codifica le Best practice per le applicazioni RAG e fornisce interfacce standard per vari componenti necessari per la creazione di applicazioni RAG.



=== LlamaIndex

LlamaIndex è un framework di dati semplice e flessibile che consente di collegare origini dati personalizzate ad applicazioni basate su modelli di linguaggio di grandi dimensioni (LLM). Consente di acquisire dati da API, database, PDF e altro ancora tramite connettori dati flessibili. LLMS come Llama 3 e GPT-4 vengono forniti preformati su enormi set di dati pubblici, consentendo incredibili funzionalità di elaborazione del linguaggio naturale. Tuttavia, la loro utilità è limitata senza l'accesso ai vostri dati privati. LlamaIndex fornisce librerie Python e typesscript molto popolari ed è leader nel settore delle tecniche RAG (Retrieval-augmented Generation).



== Microservizi NVIDIA NEMO

NVIDIA NEMO è una piattaforma end-to-end per creare e personalizzare modelli di ai generativi di livello Enterprise che possono essere implementati ovunque, tra cloud e data center. NEMO fornisce microservizi che semplificano il processo di sviluppo e distribuzione dell'intelligenza artificiale generativa su larga scala, consentendo alle organizzazioni di collegare LLMS alle proprie origini dati aziendali. Al momento di questa stesura, i microservizi NEMO sono disponibili tramite un programma di accesso anticipato di NVIDIA.



=== NVIDIA NEMO Inference Microservices (NIMS)

NVIDIA NIMS, che fa parte di NVIDIA ai Enterprise, fornisce un percorso semplificato per lo sviluppo di applicazioni aziendali basate sull'intelligenza artificiale e l'implementazione di modelli ai in produzione. NIMS è un microservizio di inferenza containerizzato che include API standard di settore, codice specifico del dominio, motori di inferenza ottimizzati e runtime Enterprise.



=== NVIDIA NEMO Retriever

NVIDIA NEMO Retriever, il servizio più recente del framework NVIDIA NEMO, ottimizza la parte di inclusione e recupero di RAG per garantire una maggiore precisione e risposte più efficienti. NVIDIA NEMO Retriever è un servizio di recupero delle informazioni che può essere implementato on-premise o nel cloud. Offre un percorso sicuro e semplificato per le aziende che integrano le funzionalità RAG di livello Enterprise nelle loro applicazioni ai di produzione personalizzate.



== Operatore NVIDIA Enterprise RAG LLM

L'operatore NVIDIA Enterprise Retrieval Augmented Generation (RAG) Large Language Model (LLM) abilita i componenti software e i servizi necessari per eseguire le pipeline RAG in Kubernetes. Fornisce accesso anticipato a un operatore che gestisce il ciclo di vita dei componenti chiave per le pipeline RAG, come NVIDIA Inference Microservice e NVIDIA NEMO Retriever Embedding Microservice. Per ulteriori informazioni, fare riferimento a. https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html["Operatore NVIDIA Enterprise RAG LLM"^]



== Database vettoriale



=== PostgreSQL: Pgvector

Con i suoi binding nativi per molti algoritmi ML classici come XGBoost, l'apprendimento automatico con SQL non è una novità di PostgreSQL. Ultimamente, con il rilascio di pgvector, un'estensione open source per la ricerca di similarità vettoriale, PostgreSQL ha la capacità di memorizzare e ricercare embeddings generati da ML, una funzionalità utile per i casi di utilizzo ai e le applicazioni che utilizzano LLMS.

La pipeline campione predefinita nella nostra convalida con l'operatore NVIDIA Enterprise RAG LLM avvia il database pgvector in un pod. Il server di query si connette quindi al database pgvector per memorizzare e recuperare le embeddings. L'applicazione web del bot di chat e il server di query comunicano con i microservizi e il database vettoriale per rispondere alle richieste dell'utente.



=== Milvus

In qualità di database vettoriale versatile che offre un'API, proprio come MongoDB, Milvus si distingue per il supporto per un'ampia varietà di tipi di dati e funzionalità come la multi-vettorizzazione, rendendola una scelta popolare per la data science e l'apprendimento automatico. Ha la capacità di memorizzare, indicizzare e gestire oltre un miliardo di vettori incorporati generati dai modelli Deep Neural Networks (DNN) e Machine Learning (ML). I clienti possono creare un'applicazione RAG utilizzando Nvidia NIM & NEMO microservice e Milvus come database vettoriale. Una volta che il contenitore NEMO NVIDIA viene implementato correttamente per la generazione di incorporamento, il contenitore Milvus può essere implementato per la conservazione di tali incorporazioni. Per ulteriori informazioni sui database vettoriali e su NetApp, vedere https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html["Architettura di riferimento: Soluzione di database vettoriale con NetApp"^].



=== Apache Cassandra

Apache Cassandra®, un database NoSQL open source altamente scalabile e ad alta disponibilità. Viene fornito con funzionalità di ricerca vettoriale e supporta tipi di dati vettoriali e funzioni di ricerca della similarità vettoriale, particolarmente utili per applicazioni ai che coinvolgono LLMS e pipeline RAG private.

NetApp Instaclustr fornisce un servizio completamente gestito per Apache Cassandra®, ospitato sia nel cloud che in sede. Consente ai clienti NetApp di eseguire il provisioning di un cluster Apache Cassandra® e di connettersi al cluster utilizzando C#, Node.js, AWS PrivateLink e varie altre opzioni tramite la console Instaclustr o l'API di provisioning di Instaclstr.

Inoltre, NetApp ONTAP agisce come provider di storage persistente per il cluster Apache Cassandra in container eseguito su Kubernetes. NetApp Astra Control estende perfettamente i vantaggi della gestione dei dati di ONTAP alle applicazioni Kubernetes ricche di dati come Apache Cassandra. Per ulteriori informazioni, fare riferimento a. https://cloud.netapp.com/hubfs/SB-4134-0321-DataStax-Cassandra-Guide%20(1).pdf["Gestione dei dati integrata con l'applicazione per DataStax Enterprise con NetApp Astra Control e storage ONTAP"^]



=== Installazione NetApp

Instaclustr aiuta le organizzazioni a fornire applicazioni su larga scala supportando la propria infrastruttura dati attraverso la piattaforma SaaS per le tecnologie open source. Gli sviluppatori di intelligenza artificiale generativa che desiderano incorporare la comprensione semantica nelle loro applicazioni di ricerca hanno una moltitudine di opzioni. Instaclustr for Postgres supporta le estensioni pgvector. Instaclustr for OpenSearch supporta la ricerca vettoriale per recuperare i documenti pertinenti in base alle query di input e alle funzioni vicine più vicine. Instaclustr for Redis può memorizzare dati vettoriali, recuperare vettori ed eseguire ricerche vettoriali. Per ulteriori informazioni, leggere https://www.instaclustr.com/platform/["La piattaforma Instaclustr di NetApp"^]



== NetApp BlueXP

NetApp BlueXP unifica tutti i servizi dati e storage di NetApp in un singolo tool che ti consente di creare, proteggere e gestire il tuo ambiente dati multicloud ibrido. Offre un'esperienza unificata per lo storage e i servizi dati in ambienti on-premise e cloud e abilita la semplicità operativa attraverso la potenza di AIOps, con i parametri di consumo flessibili e la protezione integrata richiesti per il mondo di oggi basato sul cloud.



== NetApp Cloud Insights

NetApp Cloud Insights è uno strumento di monitoraggio dell'infrastruttura cloud che offre visibilità sull'intera infrastruttura. Con Cloud Insights, puoi monitorare, risolvere i problemi e ottimizzare tutte le risorse, inclusi i cloud pubblici e i data center privati. Cloud Insights offre visibilità full-stack di infrastruttura e applicazioni da centinaia di raccoglitori per infrastruttura e workload eterogenei, inclusi Kubernetes, il tutto in un unico punto. Per ulteriori informazioni, fare riferimento a. https://docs.netapp.com/us-en/cloudinsights/index.html["Cosa può fare Cloud Insights per me?"^]



== NetApp StorageGRID

NetApp StorageGRID è una suite di storage a oggetti software-defined che supporta un'ampia gamma di casi di utilizzo in ambienti multicloud pubblici, privati e ibridi. StorageGRID offre il supporto nativo per l'API Amazon S3 e offre innovazioni leader del settore come la gestione automatica del ciclo di vita per memorizzare, proteggere, proteggere e conservare i dati non strutturati in modo conveniente per lunghi periodi.



== Spot NetApp

Spot di NetApp automatizza e ottimizza la tua infrastruttura cloud in AWS, Azure o Google Cloud per offrire la disponibilità e le performance garantite dagli SLA al minor costo possibile. Spot utilizza l'apprendimento automatico e gli algoritmi di analytics che ti consentono di utilizzare la capacità di tale prodotto per la produzione e i carichi di lavoro mission-critical. I clienti che eseguono istanze basate su GPU possono trarre beneficio da Spot e ridurre i costi di calcolo.



== NetApp ONTAP

ONTAP 9, l'ultima generazione di software per la gestione dello storage NetApp, consente alle aziende di modernizzare l'infrastruttura e passare a un data center predisposto per il cloud. Sfruttando le funzionalità di gestione dei dati leader del settore, ONTAP consente la gestione e la protezione dei dati con un singolo set di strumenti, indipendentemente dalla posizione dei dati. Puoi anche spostare liberamente i dati ovunque siano necessari: Edge, core o cloud. ONTAP 9 include numerose funzionalità che semplificano la gestione dei dati, accelerano e proteggono i dati critici e abilitano le funzionalità dell'infrastruttura di nuova generazione nelle architetture di cloud ibrido.



=== Semplifica la gestione dei dati

La gestione dei dati è fondamentale per le operazioni IT aziendali e per i data scientist, in modo che le risorse appropriate vengano utilizzate per le applicazioni ai e per la formazione dei set di dati ai/ML. Le seguenti informazioni aggiuntive sulle tecnologie NetApp non rientrano nell'ambito di questa convalida, ma potrebbero essere rilevanti a seconda dell'implementazione.

Il software per la gestione dei dati ONTAP include le seguenti funzionalità per ottimizzare e semplificare le operazioni e ridurre il costo totale delle operazioni:

* Compaction dei dati inline e deduplica estesa. La compattazione dei dati riduce lo spazio sprecato all'interno dei blocchi di storage e la deduplica aumenta significativamente la capacità effettiva. Ciò vale per i dati memorizzati localmente e per i dati a più livelli nel cloud.
* Qualità del servizio (AQoS) minima, massima e adattativa. I controlli granulari della qualità del servizio (QoS) aiutano a mantenere i livelli di performance per le applicazioni critiche in ambienti altamente condivisi.
* NetApp FabricPool. Offre il tiering automatico dei dati cold per le opzioni di cloud storage pubblico e privato, tra cui Amazon Web Services (AWS), Azure e la soluzione di storage NetApp StorageGRID. Per ulteriori informazioni su FabricPool, vedere https://www.netapp.com/pdf.html?item=/media/17239-tr4598pdf.pdf["TR-4598: Best practice FabricPool"^].




=== Accelera e proteggi i dati

ONTAP offre livelli superiori di performance e protezione dei dati ed estende queste funzionalità nei seguenti modi:

* Performance e latenza ridotta. ONTAP offre il throughput più elevato possibile con la latenza più bassa possibile.
* Protezione dei dati. ONTAP offre funzionalità di protezione dei dati integrate con gestione comune su tutte le piattaforme.
* NetApp Volume Encryption (NVE). ONTAP offre crittografia nativa a livello di volume con supporto per la gestione delle chiavi sia integrata che esterna.
* Multi-tenancy e autenticazione a più fattori. ONTAP consente la condivisione delle risorse dell'infrastruttura con i massimi livelli di sicurezza.




=== Infrastruttura a prova di futuro

ONTAP aiuta a soddisfare le esigenze di business esigenti e in continua evoluzione con le seguenti funzionalità:

* Scalabilità perfetta e operazioni senza interruzioni. ONTAP supporta l'aggiunta senza interruzioni di capacità ai controller esistenti e ai cluster scale-out. I clienti possono eseguire l'upgrade alle tecnologie più recenti, come NVMe e 32GB FC, senza costose migrazioni dei dati o interruzioni.
* Connessione al cloud. ONTAP è il software per la gestione dello storage più connesso al cloud, con opzioni per storage software-defined e istanze native per il cloud in tutti i cloud pubblici.
* Integrazione con le applicazioni emergenti. ONTAP offre servizi dati di livello Enterprise per piattaforme e applicazioni di prossima generazione, come veicoli autonomi, città intelligenti e industria 4.0, utilizzando la stessa infrastruttura che supporta le applicazioni aziendali esistenti.




== Amazon FSX per NetApp ONTAP

Amazon FSX per NetApp ONTAP è un servizio AWS completamente gestito e first-party che offre un file storage altamente affidabile, scalabile, dalle performance elevate e ricco di funzionalità, costruito sul popolare file system ONTAP di NetApp. FSX per ONTAP combina le funzionalità, le performance, le funzionalità e le operazioni API dei file system NetApp con l'agilità, la scalabilità e la semplicità di un servizio AWS completamente gestito.



== Azure NetApp Files

Azure NetApp Files è un servizio di file storage nativo, first-party, di livello Enterprise e dalle performance elevate di Azure. Supporta volumi SMB, NFS e protocolli doppi e può essere utilizzato per casi d'utilizzo come:

* Condivisione dei file.
* Home directory.
* Database.
* High-performance computing
* Ai generativa.




== Google Cloud NetApp Volumes

Google Cloud NetApp Volumes è un servizio di storage dei dati basato sul cloud e completamente gestito, che offre capacità avanzate di gestione dei dati e performance altamente scalabili. I dati ospitati da NetApp possono essere utilizzati nelle operazioni RAG (recupero-generazione aumentata) per la piattaforma Vertex ai di Google in un'architettura di riferimento toolkit visualizzata in anteprima.



== NetApp Astra Trident

Astra Trident permette il consumo e la gestione delle risorse di storage in tutte le più apprezzate piattaforme di storage NetApp, nel cloud pubblico o on-premise, incluso ONTAP (AFF, FAS, Select, cloud, Amazon FSX per NetApp ONTAP), software Element (NetApp HCI, SolidFire), servizio Azure NetApp Files e Cloud Volumes Service su Google Cloud. Astra Trident è un orchestrator di storage dinamico conforme a Container Storage Interface (CSI) che si integra in modo nativo con Kubernetes.



== Kubernetes

Kubernetes è una piattaforma open source, distribuita e di orchestrazione dei container, originariamente progettata da Google e ora gestita dalla Cloud Native Computing Foundation (CNCF). Kubernetes offre l'automazione delle funzioni di implementazione, gestione e scalabilità per le applicazioni in container ed è la piattaforma di orchestrazione dei container dominante negli ambienti Enterprise.
