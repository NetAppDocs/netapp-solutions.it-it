---
sidebar: sidebar 
permalink: ai/osrunai_executive_summary.html 
keywords:  
summary:  
---
= TR-4858: Soluzione di orchestrazione NetApp con Run:ai
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


Rick Huang, David Arnette, Sung-Han Lin, NetApp Yaron Goldberg, Run:ai

[role="lead"]
I sistemi storage NetApp AFF offrono performance estreme e funzionalità di gestione dei dati del cloud ibrido leader di settore. NetApp e Run:ai hanno collaborato per dimostrare le funzionalità esclusive della soluzione NetApp ONTAP ai per i carichi di lavoro di intelligenza artificiale (ai) e machine learning (ML) che offrono performance, affidabilità e supporto di livello Enterprise. Run:l'orchestrazione ai dei carichi di lavoro ai aggiunge una piattaforma di scheduling e utilizzo delle risorse basata su Kubernetes per aiutare i ricercatori a gestire e ottimizzare l'utilizzo della GPU. Insieme ai sistemi NVIDIA DGX, la soluzione combinata di NetApp, NVIDIA e Run:ai offre uno stack di infrastruttura costruito ad hoc per i carichi di lavoro ai aziendali. Questo report tecnico fornisce una guida direzionale ai clienti che sviluppano sistemi di ai conversazionali a supporto di vari casi di utilizzo e mercati verticali del settore. Include informazioni sull'implementazione di Run:ai e di un sistema storage NetApp AFF A800 e funge da architettura di riferimento per il modo più semplice per ottenere un'implementazione rapida e di successo delle iniziative ai.

Il pubblico di riferimento per la soluzione comprende i seguenti gruppi:

* Architetti aziendali che progettano soluzioni per lo sviluppo di modelli ai e software per casi di utilizzo basati su Kubernetes, come i microservizi containerizzati
* Data scientist alla ricerca di modi efficienti per raggiungere obiettivi di sviluppo dei modelli efficienti in un ambiente cluster con più team e progetti
* Data engineer responsabili della manutenzione e dell'esecuzione dei modelli di produzione
* Decision maker e dirigenti IT e business leader che desiderano creare l'esperienza ottimale di utilizzo delle risorse cluster di Kubernetes e ottenere il più rapido time-to-market dalle iniziative di ai

