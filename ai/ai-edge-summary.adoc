---
sidebar: sidebar 
permalink: ai/ai-edge-summary.html 
keywords:  
summary:  
---
= Riepilogo
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Diversi scenari applicativi emergenti, come i sistemi avanzati di assistenza alla guida (ADAS), Industry 4.0, smart cities e Internet of Things (IoT), richiedono l'elaborazione di flussi di dati continui con una latenza quasi nulla. Questo documento descrive un'architettura di calcolo e storage per implementare deduzione di intelligenza artificiale (ai) basata su GPU su storage controller NetApp e server Lenovo ThinkSystem in un ambiente edge che soddisfi questi requisiti. Questo documento fornisce inoltre dati sulle performance per il benchmark MLPerf Inference standard di settore, valutando varie attività di inferenza su edge server dotati di GPU NVIDIA T4. Analizziamo le performance degli scenari di inferenza offline, single stream e multistream e mostriamo che l'architettura con un sistema di storage condiviso in rete a costi contenuti è altamente performante e fornisce un punto centrale per la gestione di dati e modelli per più edge server.
