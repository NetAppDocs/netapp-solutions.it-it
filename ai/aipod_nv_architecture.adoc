---
sidebar: sidebar 
permalink: ai/aipod_nv_architecture.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: Pod NetApp ai con sistemi NVIDIA DGX - architettura 
---
= Pod NetApp ai con sistemi NVIDIA DGX - architettura
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:aipod_nv_sw_components.html["Precedente: ONTAP ai - componenti software."]

Questa architettura di riferimento sfrutta fabric separati per l'interconnessione del cluster di calcolo e l'accesso allo storage, con opzioni per la connettività NDR200 GbE e HDR200 GB Infiniband (IB) tra nodi di calcolo. I sistemi DGX H100 sono dotati di schede ConnectX-7 preinstallate per la connettività IB NDR, mentre i sistemi DGX A100 possono utilizzare schede ConnectX-6 o ConnectX-7 per la connettività HDR o NDR, rispettivamente.



== ONTAP ai con sistemi DGX H100

Il disegno qui sotto mostra la topologia della soluzione globale quando si utilizzano sistemi DGX H100 con ONTAP ai.

image:oai_H100_topo.png["Errore: Immagine grafica mancante"]

In questa configurazione la rete cluster di elaborazione utilizza una coppia di switch QM9700 NDR IB, che sono connessi insieme per l'alta disponibilità. Ogni sistema DGX H100 è collegato agli switch utilizzando otto connessioni NDR200, con porte con numero pari collegate a uno switch e porte con numero dispari collegate all'altro switch.

Per l'accesso al sistema di storage, la gestione in banda e l'accesso client, viene utilizzata una coppia di switch Ethernet SN4600. Gli switch sono collegati con collegamenti inter-switch e configurati con più VLAN per isolare i vari tipi di traffico. Per implementazioni di dimensioni maggiori, la rete ethernet può essere estesa a una configurazione leaf-spine aggiungendo ulteriori coppie di switch per una colonna vertebrale e altri fogli in base alle necessità. Ogni sistema DGX A100 viene fornito con due schede ConnectX-6 a due porte per il traffico ethernet e di storage e per questa soluzione tutte e quattro le porte sono connesse ai SN4600 switch Ethernet a 200 Gbps. Una porta di ogni scheda è configurata in un legame LACP MLAG con una porta collegata a ogni switch, e VLAN per la gestione in banda, l'accesso client e l'accesso storage a livello utente sono ospitati su questo legame. L'altra porta di ciascuna scheda viene utilizzata in maniera indipendente in VLAN di storage RoCE dedicate separate per la connettività al sistema di storage AFF A800. Queste porte supportano l'accesso allo storage dalle performance elevate mediante NFS v3, NFSv4.x con pNFS e NFS su RDMA.

Oltre all'interconnessione di elaborazione e alle reti ethernet ad alta velocità, tutti i dispositivi fisici sono collegati anche a uno o più switch Ethernet SN2201 per la gestione fuori banda.  Per ulteriori dettagli sulla connettività del sistema DGX A100, fare riferimento al link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["Documentazione NVIDIA BasePOD"].



== ONTAP ai con sistemi DGX A100

Il disegno qui sotto mostra la topologia della soluzione generale quando si utilizzano sistemi DGX A100 e un fabric di calcolo HDR con ONTAP ai.

image:oai_A100_topo.png["Errore: Immagine grafica mancante"]

In questa configurazione, la rete cluster di elaborazione utilizza una coppia di switch QM8700 HDR IB, che sono collegati insieme per garantire l'alta disponibilità. Ogni sistema DGX A100 è collegato agli switch utilizzando quattro schede ConnectX-6 a una porta a 200 Gbps, con porte pari collegate a uno switch e porte con numeri dispari collegate all'altro switch.

Per l'accesso al sistema di storage, la gestione in banda e l'accesso client, viene utilizzata una coppia di switch Ethernet SN4600. Gli switch sono collegati con collegamenti inter-switch e configurati con più VLAN per isolare i vari tipi di traffico. Per implementazioni di dimensioni maggiori, la rete ethernet può essere estesa a una configurazione leaf-spine aggiungendo ulteriori coppie di switch per una colonna vertebrale e altri fogli in base alle necessità. Ogni sistema DGX A100 viene fornito con due schede ConnectX-6 a due porte per il traffico ethernet e di storage e per questa soluzione tutte e quattro le porte sono connesse ai SN4600 switch Ethernet a 200 Gbps. Una porta di ogni scheda è configurata in un legame LACP MLAG con una porta collegata a ogni switch, e VLAN per la gestione in banda, l'accesso client e l'accesso storage a livello utente sono ospitati su questo legame. L'altra porta di ciascuna scheda viene utilizzata in maniera indipendente in VLAN di storage RoCE dedicate separate per la connettività al sistema di storage AFF A800. Queste porte supportano l'accesso allo storage dalle performance elevate mediante NFS v3, NFSv4.x con pNFS e NFS su RDMA.

Oltre all'interconnessione di elaborazione e alle reti ethernet ad alta velocità, tutti i dispositivi fisici sono collegati anche a uno o più switch Ethernet SN2201 per la gestione fuori banda.  Per ulteriori dettagli sulla connettività del sistema DGX A100, fare riferimento al link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["Documentazione NVIDIA BasePOD"].



== Server piani di gestione

Questa architettura di riferimento include anche cinque server basati su CPU per gli utilizzi dei piani di gestione. Due di questi sistemi sono utilizzati come nodi principali per il base Command Manager per l'implementazione e la gestione del cluster. Gli altri tre sistemi sono utilizzati per fornire servizi cluster aggiuntivi come nodi master Kubernetes o nodi di login per implementazioni che utilizzano Slurm per la pianificazione dei processi. Le implementazioni che utilizzano Kubernetes possono sfruttare il driver NetApp Astra Trident CSI per fornire provisioning automatizzato e servizi dati con storage persistente per carichi di lavoro di gestione e ai sul sistema storage AFF A800.

Ciascun server è fisicamente collegato sia agli switch IB che agli switch ethernet per consentire l'implementazione e la gestione del cluster e configurato con mount NFS al sistema storage tramite la SVM di gestione per lo storage degli artefatti di gestione dei cluster, come descritto in precedenza.

link:aipod_nv_storage.html["Pagina successiva: Pod NetApp ai con sistemi NVIDIA DGX - Guida al dimensionamento e alla progettazione del sistema storage."]
