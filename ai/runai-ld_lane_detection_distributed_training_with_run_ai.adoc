---
sidebar: sidebar 
permalink: ai/runai-ld_lane_detection_distributed_training_with_run_ai.html 
keywords: azure, lane, detection, training, case, tusimple, dataset, aks, subnet, virtual, network, run, ai, deploy, install, download, process, back, end, storage, horovod, snapshot 
summary: 'Questa sezione fornisce dettagli sulla configurazione della piattaforma per l"esecuzione di training distribuiti di rilevamento di corsia su larga scala utilizzando L"orchestrator DI RUN ai.' 
---
= Lane Detection – formazione distribuita con RUN:ai
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Questa sezione fornisce dettagli sulla configurazione della piattaforma per l'esecuzione del training distribuito di rilevamento della corsia su larga scala utilizzando L'ORCHESTRATOR DI intelligenza artificiale. Discutiamo dell'installazione di tutti gli elementi della soluzione e dell'esecuzione del lavoro di training distribuito sulla piattaforma suddetta. IL controllo della versione ML viene completato utilizzando NetApp SnapshotTM collegato a ESPERIMENTI DI RUN: Ai per ottenere la riproducibilità dei dati e dei modelli. IL controllo delle versioni DI ML svolge un ruolo cruciale nel monitoraggio dei modelli, nella condivisione del lavoro tra i membri del team, nella riproducibilità dei risultati, nel passaggio in produzione delle nuove versioni dei modelli e nella provenienza dei dati. NetApp ML version control (Snapshot) è in grado di acquisire versioni point-in-time dei dati, dei modelli addestrati e dei registri associati a ciascun esperimento. Grazie al supporto API completo, è facile da integrare con LA piattaforma DI ESECUZIONE: Ai; devi solo attivare un evento in base allo stato del training. Inoltre, è necessario acquisire lo stato dell'intero esperimento senza modificare nulla nel codice o nei container eseguiti su Kubernetes (K8s).

Infine, questo report tecnico si conclude con la valutazione delle performance su più nodi abilitati alla GPU in AKS.



== Training distribuito per il caso di utilizzo del rilevamento di corsia utilizzando il set di dati TuSimple

In questo report tecnico, viene eseguito un training distribuito sul set di dati TuSimple per il rilevamento della corsia. Horovod viene utilizzato nel codice di training per condurre training distribuiti su più nodi GPU contemporaneamente nel cluster Kubernetes tramite AKS. Il codice viene confezionato come immagini container per il download e l'elaborazione dei dati TuSimple. I dati elaborati vengono memorizzati su volumi persistenti allocati dal plug-in di NetApp Trident. Per il training, viene creata un'altra immagine container che utilizza i dati memorizzati nei volumi persistenti creati durante il download dei dati.

Per inviare i dati e il lavoro di training, utilizza RUN: Ai per orchestrare l'allocazione e la gestione delle risorse. ESEGUI: L'ai consente di eseguire operazioni MPI (message Passing Interface) necessarie per Horovod. Questo layout consente a più nodi GPU di comunicare tra loro per aggiornare i pesi di training dopo ogni mini batch di training. Consente inoltre di monitorare la formazione attraverso l'interfaccia utente e la CLI, semplificando il monitoraggio dei progressi degli esperimenti.

NetApp Snapshot è integrato nel codice di training e acquisisce lo stato dei dati e il modello formativo per ogni esperimento. Questa funzionalità consente di tenere traccia della versione dei dati e del codice utilizzati e del modello di formazione associato generato.



== Installazione e configurazione di AKS

Per la configurazione e l'installazione del cluster AKS, visitare il sito Web all'indirizzo https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal["Creare un cluster AKS"^]. Quindi, attenersi alla seguente serie di passaggi:

. Quando si seleziona il tipo di nodi (che si tratti di nodi di sistema (CPU) o di lavoro (GPU)), selezionare quanto segue:
+
.. Aggiungere il nodo di sistema primario denominato `agentpool` su `Standard_DS2_v2` dimensione. Utilizzare i tre nodi predefiniti.
.. Aggiungi nodo di lavoro `gpupool` con `the Standard_NC6s_v3` dimensioni del pool. Utilizzare almeno tre nodi per i nodi GPU.
+
image::runai-ld_image3.png[runai ld image3]

+

NOTE: L'implementazione richiede 10 minuti.



. Al termine dell'implementazione, fare clic su Connect to Cluster (Connetti al cluster). Per connettersi al cluster AKS appena creato, installare il tool della riga di comando Kubernetes dall'ambiente locale (laptop/PC). Visitare il sito https://kubernetes.io/docs/tasks/tools/install-kubectl/["Strumenti di installazione"^] Per installarlo in base al sistema operativo in uso.
. https://docs.microsoft.com/cli/azure/install-azure-cli["Installare Azure CLI nell'ambiente locale"^].
. Per accedere al cluster AKS dal terminale, immettere `az login` e inserire le credenziali.
. Eseguire i due comandi seguenti:
+
....
az account set --subscription xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxx
aks get-credentials --resource-group resourcegroup --name aksclustername
....
. Immettere questo comando nella riga di comando Azure:
+
....
kubectl get nodes
....
+

NOTE: Se tutti e sei i nodi sono attivi e in esecuzione, il cluster AKS è pronto e connesso all'ambiente locale.

+
image::runai-ld_image4.png[runai ld image4]





== Creare una subnet delegata per Azure NetApp Files

Per creare una subnet delegata per Azure NetApp Files, seguire questa serie di passaggi:

. Accedere alle reti virtuali all'interno del portale Azure. Trova la tua rete virtuale appena creata. Dovrebbe avere un prefisso come aks-vnet, come mostrato qui. Fare clic sul nome della rete virtuale.
+
image::runai-ld_image5.png[runai ld image5]

. Fare clic su subnet e selezionare +Subnet nella barra degli strumenti superiore.
+
image::runai-ld_image6.png[runai ld image6]

. Specificare un nome per la subnet, ad esempio `ANF.sn` E sotto l'intestazione Subnet Delegation (delega subnet), selezionare Microsoft.NetApp/volumes. Non cambiare altro. Fare clic su OK.
+
image::runai-ld_image7.png[runai ld image7]



I volumi Azure NetApp Files vengono allocati nel cluster di applicazioni e vengono utilizzati come dichiarazioni di volumi persistenti (PVC) in Kubernetes. A sua volta, questa allocazione ci offre la flessibilità di mappare i volumi a diversi servizi, sia che si trattino di notebook Jupyter, funzioni senza server e così via

Gli utenti dei servizi possono consumare lo storage dalla piattaforma in molti modi. I principali vantaggi di Azure NetApp Files sono:

* Offre agli utenti la possibilità di utilizzare le snapshot.
* Consente agli utenti di memorizzare grandi quantità di dati su volumi Azure NetApp Files.
* Ottenere i vantaggi in termini di performance dei volumi Azure NetApp Files quando si eseguono i modelli su grandi set di file.




== Configurazione di Azure NetApp Files

Per completare la configurazione di Azure NetApp Files, è necessario configurarla come descritto in https://docs.microsoft.com/azure/azure-netapp-files/azure-netapp-files-quickstart-set-up-account-create-volumes["QuickStart: Configurazione di Azure NetApp Files e creazione di un volume NFS"^].

Tuttavia, è possibile omettere la procedura per creare un volume NFS per Azure NetApp Files, poiché si creeranno volumi tramite Trident. Prima di continuare, assicurarsi di disporre di:

. https://docs.microsoft.com/azure/azure-netapp-files/azure-netapp-files-register["Registrato per Azure NetApp Files e per il provider di risorse NetApp (tramite la shell cloud di Azure)"^].
. https://docs.microsoft.com/azure/azure-netapp-files/azure-netapp-files-create-netapp-account["Creato un account in Azure NetApp Files"^].
. https://docs.microsoft.com/en-us/azure/azure-netapp-files/azure-netapp-files-set-up-capacity-pool["Impostare un pool di capacità"^] (Minimo 4 TiB Standard o Premium a seconda delle esigenze).




== Peering della rete virtuale AKS e della rete virtuale Azure NetApp Files

Quindi, eseguire il peer della rete virtuale AKS con Azure NetApp Files VNET seguendo questa procedura:

. Nella casella di ricerca nella parte superiore del portale Azure, digitare virtual networks (reti virtuali).
. Fare clic su VNET aks- vnet-name, quindi immettere Peerings nel campo di ricerca.
. Fare clic su +Add (Aggiungi) e inserire le informazioni fornite nella tabella seguente:
+
|===


| Campo | Valore o descrizione # 


| Nome del collegamento peering | aks-vnet-name_to_an 


| SubscriptionID | Iscrizione a Azure NetApp Files VNET a cui stai eseguendo il peering 


| Partner di peering VNET | Azure NetApp Files VNET 
|===
+

NOTE: Lasciare tutte le sezioni non contrassegnate come predefinite

. Fare clic su ADD (AGGIUNGI) o su OK per aggiungere il peering alla rete virtuale.


Per ulteriori informazioni, visitare il sito https://docs.microsoft.com/azure/virtual-network/tutorial-connect-virtual-networks-portal["Creare, modificare o eliminare un peering di rete virtuale"^].



== Trident

Trident è un progetto open-source che NetApp gestisce per lo storage persistente dei container delle applicazioni. Trident è stato implementato come un provisioning controller esterno che viene eseguito come pod stesso, monitorando i volumi e automatizzando completamente il processo di provisioning.

NetApp Trident consente un'integrazione perfetta con K8s creando e allegando volumi persistenti per l'archiviazione di set di dati di training e modelli di training. Questa funzionalità semplifica l'utilizzo di K8 da parte di data scientist e data engineer senza il fastidio di memorizzare e gestire manualmente i set di dati. Trident elimina inoltre la necessità per i data scientist di imparare a gestire nuove piattaforme dati, poiché integra le attività correlate alla gestione dei dati attraverso l'integrazione API logica.



=== Installare Trident

Per installare il software Trident, attenersi alla seguente procedura:

. https://helm.sh/docs/intro/install/["Installare prima il timone"^].
. Scaricare ed estrarre il programma di installazione di Trident 21.01.1.
+
....
wget https://github.com/NetApp/trident/releases/download/v21.01.1/trident-installer-21.01.1.tar.gz
tar -xf trident-installer-21.01.1.tar.gz
....
. Modificare la directory in `trident-installer`.
+
....
cd trident-installer
....
. Copia `tridentctl` a una directory del sistema `$PATH.`
+
....
cp ./tridentctl /usr/local/bin
....
. Installare Trident sul cluster K8s con Helm:
+
.. Cambiare la directory in Helm directory.
+
....
cd helm
....
.. Installare Trident.
+
....
helm install trident trident-operator-21.01.1.tgz --namespace trident --create-namespace
....
.. Verificare lo stato dei pod Trident nel modo consueto di K8s:
+
....
kubectl -n trident get pods
....
.. Se tutti i pod sono in funzione, Trident è installato e si è bene andare avanti.






== Configurare il back-end Azure NetApp Files e la classe di storage

Per configurare il back-end Azure NetApp Files e la classe di storage, attenersi alla seguente procedura:

. Tornare alla home directory.
+
....
cd ~
....
. Clonare il https://github.com/dedmari/lane-detection-SCNN-horovod.git["repository di progetto"^] `lane-detection-SCNN-horovod`.
. Accedere alla `trident-config` directory.
+
....
cd ./lane-detection-SCNN-horovod/trident-config
....
. Creare un principio di servizio Azure (il principio di servizio è il modo in cui Trident comunica con Azure per accedere alle risorse Azure NetApp Files).
+
....
az ad sp create-for-rbac --name
....
+
L'output dovrebbe essere simile al seguente esempio:

+
....
{
  "appId": "xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
   "displayName": "netapptrident",
    "name": "http://netapptrident",
    "password": "xxxxxxxxxxxxxxx.xxxxxxxxxxxxxx",
    "tenant": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx"
 }
....
. Creare il Trident `backend json` file.
. Utilizzando l'editor di testo preferito, completare i seguenti campi della tabella riportata di seguito all'interno di `anf-backend.json` file.
+
|===
| Campo | Valore 


| SubscriptionID | Il tuo ID di abbonamento Azure 


| ID tenant | Il tuo ID tenant Azure (dall'output di az ad sp nel passaggio precedente) 


| ID cliente | Il tuo appID (dall'output di az ad sp nel passaggio precedente) 


| ClientSecret | La tua password (dall'output di az ad sp nel passaggio precedente) 
|===
+
Il file dovrebbe essere simile al seguente esempio:

+
....
{
    "version": 1,
    "storageDriverName": "azure-netapp-files",
    "subscriptionID": "fakec765-4774-fake-ae98-a721add4fake",
    "tenantID": "fakef836-edc1-fake-bff9-b2d865eefake",
    "clientID": "fake0f63-bf8e-fake-8076-8de91e57fake",
    "clientSecret": "SECRET",
    "location": "westeurope",
    "serviceLevel": "Standard",
    "virtualNetwork": "anf-vnet",
    "subnet": "default",
    "nfsMountOptions": "vers=3,proto=tcp",
    "limitVolumeSize": "500Gi",
    "defaults": {
    "exportRule": "0.0.0.0/0",
    "size": "200Gi"
}
....
. Chiedere a Trident di creare il back-end Azure NetApp Files in `trident` namespace, utilizzando `anf-backend.json` come il file di configurazione come segue:
+
....
tridentctl create backend -f anf-backend.json -n trident
....
. Creare la classe di storage:
+
.. Gli utenti K8 eseguono il provisioning dei volumi utilizzando PVC che specificano una classe di storage in base al nome. Chiedere a K8s di creare una classe di storage `azurenetappfiles` Questo farà riferimento al back-end Azure NetApp Files creato nel passaggio precedente utilizzando quanto segue:
+
....
kubectl create -f anf-storage-class.yaml
....
.. Verificare che la classe di storage venga creata utilizzando il seguente comando:
+
....
kubectl get sc azurenetappfiles
....
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image8.png[runai ld image8]







== Implementare e configurare i componenti di snapshot dei volumi su AKS

Se il cluster non viene fornito con i componenti di snapshot del volume corretti, è possibile installare manualmente questi componenti eseguendo i seguenti passaggi:


NOTE: AKS 1.18.14 non dispone di Snapshot Controller preinstallato.

. Installare i CRD Snapshot Beta utilizzando i seguenti comandi:
+
....
kubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-3.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-3.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-3.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
....
. Installare Snapshot Controller utilizzando i seguenti documenti di GitHub:
+
....
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-3.0/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-3.0/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
....
. Impostare K8s `volumesnapshotclass`Prima di creare uno snapshot di volume https://netapp-trident.readthedocs.io/en/stable-v20.01/kubernetes/concepts/objects.html["classe di snapshot del volume"^] deve essere configurato. Creare una classe di snapshot di volume per Azure NetApp Files e utilizzarla per ottenere IL controllo delle versioni ML utilizzando la tecnologia NetApp Snapshot. Creare `volumesnapshotclass netapp-csi-snapclass` e impostarlo sul valore predefinito `volumesnapshotclass `come tale:
+
....
kubectl create -f netapp-volume-snapshot-class.yaml
....
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image9.png[runai ld image9]

. Verificare che la classe di copia Snapshot del volume sia stata creata utilizzando il seguente comando:
+
....
kubectl get volumesnapshotclass
....
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image10.png[runai ld image10]





== ESEGUI:installazione ai

Per installare RUN:ai, attenersi alla seguente procedura:

. https://docs.run.ai/Administrator/Cluster-Setup/cluster-install/["Installare IL cluster RUN:ai su AKS"^].
. Accedere a app.runai.ai, fare clic su Create New Project (Crea nuovo progetto) e assegnargli il nome di rilevamento della corsia. Verrà creato uno spazio dei nomi su un cluster K8s a partire da `runai`- seguito dal nome del progetto. In questo caso, lo spazio dei nomi creato sarà runai-lane-detection.
+
image::runai-ld_image11.png[runai ld image11]

. https://docs.run.ai/Administrator/Cluster-Setup/cluster-install/["INSTALLARE RUN:AI CLI"^].
. Sul terminale, impostare il rilevamento di corsia come UN progetto di default RUN: Ai utilizzando il seguente comando:
+
....
`runai config project lane-detection`
....
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image12.png[runai ld image12]

. Creare ClusterRole e ClusterRoleBinding per lo spazio dei nomi del progetto (ad esempio, `lane-detection)` quindi, l'account di servizio predefinito appartenente a. `runai-lane-detection` lo spazio dei nomi dispone dell'autorizzazione per eseguire le operazioni `volumesnapshot` operazioni durante l'esecuzione del processo:
+
.. Elencare gli spazi dei nomi per controllarli `runai-lane-detection` esiste utilizzando questo comando:
+
....
kubectl get namespaces
....
+
L'output dovrebbe apparire come nell'esempio seguente:

+
image::runai-ld_image13.png[runai ld image13]



. Creare ClusterRole `netappsnapshot` E ClusterRoleBinding `netappsnapshot` utilizzando i seguenti comandi:
+
....
`kubectl create -f runai-project-snap-role.yaml`
`kubectl create -f runai-project-snap-role-binding.yaml`
....




== Scaricare ed elaborare il set di dati TuSimple come lavoro RUN:ai

Il processo per scaricare ed elaborare il set di dati TuSimple come UN processo DI ESECUZIONE: Ai è facoltativo. La procedura prevede i seguenti passaggi:

. Creare e inviare l'immagine del docker o omettere questo passaggio se si desidera utilizzare un'immagine del docker esistente (ad esempio, `muneer7589/download-tusimple:1.0)`
+
.. Passare alla home directory:
+
....
cd ~
....
.. Accedere alla directory dei dati del progetto `lane-detection-SCNN-horovod`:
+
....
cd ./lane-detection-SCNN-horovod/data
....
.. Modificare `build_image.sh` shell script e modifica il repository di docker in base al tuo. Ad esempio, sostituire `muneer7589` con il nome del repository di docker. È anche possibile modificare il nome e IL TAG dell'immagine del docker (ad esempio `download-tusimple` e. `1.0`):
+
image::runai-ld_image14.png[runai ld image14]

.. Eseguire lo script per creare l'immagine del docker e inserirla nel repository del docker utilizzando i seguenti comandi:
+
....
chmod +x build_image.sh
./build_image.sh
....


. Inviare il lavoro DI ESECUZIONE: Ai per scaricare, estrarre, pre-elaborare e memorizzare il set di dati di rilevamento della corsia TuSimple in un `pvc`, Creata dinamicamente da NetApp Trident:
+
.. Utilizzare i seguenti comandi per inviare LA SERIOGRAFIA: Al job:
+
....
runai submit
--name download-tusimple-data
--pvc azurenetappfiles:100Gi:/mnt
--image muneer7589/download-tusimple:1.0
....
.. Inserire le informazioni dalla tabella seguente per inviare il job RUN:ai:
+
|===
| Campo | Valore o descrizione 


| -name | Nome del lavoro 


| pvc | PVC del formato [StorageClassName]:Size:ContainerMountPath nell'invio del job di cui sopra, si sta creando un PVC basato su richiesta utilizzando Trident con azurenetappfile di classe storage. La capacità del volume persistente qui è di 100 Gi ed è montata in path /mnt. 


| -immagine | Immagine Docker da utilizzare durante la creazione del contenitore per questo lavoro 
|===
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image15.png[runai ld image15]

.. Elencare i job RUN:ai inviati.
+
....
runai list jobs
....
+
image::runai-ld_image16.png[runai ld image16]

.. Controllare i log dei lavori inoltrati.
+
....
runai logs download-tusimple-data -t 10
....
+
image::runai-ld_image17.png[runai ld image17]

.. Elencare `pvc` creato. Utilizzare questo `pvc` comando per la formazione nella fase successiva.
+
....
kubectl get pvc | grep download-tusimple-data
....
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image18.png[runai ld image18]

.. Controllare il lavoro IN ESECUZIONE: Ai UI (o. `app.run.ai`).
+
image::runai-ld_image19.png[runai ld image19]







== Eseguire un training di rilevamento di corsia distribuito utilizzando Horovod

L'esecuzione di un training di rilevamento di corsia distribuito con Horovod è un processo facoltativo. Tuttavia, di seguito sono riportati i passaggi:

. Creare e inviare l'immagine del docker o saltare questo passaggio se si desidera utilizzare l'immagine del docker esistente (ad esempio, `muneer7589/dist-lane-detection:3.1):`
+
.. Passare alla home directory.
+
....
cd ~
....
.. Accedere alla directory del progetto `lane-detection-SCNN-horovod.`
+
....
cd ./lane-detection-SCNN-horovod
....
.. Modificare il `build_image.sh` shell script e modifica il repository di docker in base al tuo (ad esempio, sostituire `muneer7589` con il nome del repository del docker). È anche possibile modificare il nome e IL TAG dell'immagine del docker (`dist-lane-detection` e. `3.1, for example)`.
+
image::runai-ld_image20.png[runai ld image20]

.. Eseguire lo script per creare l'immagine del docker e passare al repository del docker.
+
....
chmod +x build_image.sh
./build_image.sh
....


. Inviare la CORSA: Lavoro ai per l'esecuzione del training distribuito (MPI):
+
.. Utilizzo di submit of RUN: L'ai per la creazione automatica del PVC nella fase precedente (per il download dei dati) consente solo l'accesso RWO, che non consente a più pod o nodi di accedere allo stesso PVC per la formazione distribuita. Aggiornare la modalità di accesso a ReadWriteMany e utilizzare la patch Kubernetes per eseguire questa operazione.
.. Innanzitutto, ottenere il nome del volume del PVC eseguendo il seguente comando:
+
....
kubectl get pvc | grep download-tusimple-data
....
+
image::runai-ld_image21.png[runai ld image21]

.. Applicare la patch al volume e aggiornare la modalità di accesso a ReadWriteMany (sostituire il nome del volume con il proprio nel seguente comando):
+
....
kubectl patch pv pvc-bb03b74d-2c17-40c4-a445-79f3de8d16d5 -p '{"spec":{"accessModes":["ReadWriteMany"]}}'
....
.. Inviare la CORSA: Lavoro ai MPI per l'esecuzione del lavoro di training distribuito` utilizzando le informazioni della tabella seguente:
+
....
runai submit-mpi
--name dist-lane-detection-training
--large-shm
--processes=3
--gpu 1
--pvc pvc-download-tusimple-data-0:/mnt
--image muneer7589/dist-lane-detection:3.1
-e USE_WORKERS="true"
-e NUM_WORKERS=4
-e BATCH_SIZE=33
-e USE_VAL="false"
-e VAL_BATCH_SIZE=99
-e ENABLE_SNAPSHOT="true"
-e PVC_NAME="pvc-download-tusimple-data-0"
....
+
|===
| Campo | Valore o descrizione 


| nome | Nome del lavoro di formazione distribuito 


| grande shm | Montare un grande dispositivo /dev/shm si tratta di un file system condiviso montato sulla RAM e fornisce una memoria condivisa abbastanza grande per consentire a più lavoratori della CPU di elaborare e caricare batch nella RAM della CPU. 


| processi | Numero di processi di formazione distribuiti 


| gpu | Numero di GPU/processi da allocare per il processo in questo processo, esistono tre processi di lavoro GPU (--processi=3), ciascuno allocato con una singola GPU (--gpu 1) 


| pvc | Utilizza il volume persistente esistente (pvc-download-tusemplici-data-0) creato dal job precedente (download-tusemplici-data) e viene montato nel percorso /mnt 


| immagine | Immagine Docker da utilizzare durante la creazione del contenitore per questo lavoro 


2+| Definire le variabili di ambiente da impostare nel container 


| LAVORATORI_DI_UTILIZZO | Impostando l'argomento su true si attiva il caricamento dei dati multi-processo 


| NUM_WORKERS | Numero di processi di lavoro del data loader 


| BATCH_SIZE | Dimensione del batch di training 


| VALORE_UTILIZZO | L'impostazione dell'argomento su true consente la convalida 


| VAL_BATCH_SIZE | Dimensione del batch di convalida 


| ENABLE_SNAPSHOT | Impostando l'argomento su true, è possibile acquisire dati e snapshot dei modelli con formazione per scopi di versioning ML 


| NOME_PVC | Nome del pvc di cui eseguire un'istantanea. Nell'invio del job di cui sopra, si sta prendendo un'istantanea di pvc-download-tusSimple-data-0, che consiste di dataset e modelli addestrati 
|===
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image22.png[runai ld image22]

.. Elencare il lavoro inoltrato.
+
....
runai list jobs
....
+
image::runai-ld_image23.png[runai ld image23]

.. Log dei lavori inoltrati:
+
....
runai logs dist-lane-detection-training
....
+
image::runai-ld_image24.png[runai ld image24]

.. Controllare il lavoro di training in CORSO: Ai GUI (o app.runai.ai): RUN: Ai Dashboard, come mostrato nelle figure seguenti. La prima figura descrive in dettaglio tre GPU allocate per il lavoro di training distribuito su tre nodi su AKS e la seconda ESECUZIONE:job ai:
+
image::runai-ld_image25.png[runai ld image25]

+
image::runai-ld_image26.png[runai ld image26]

.. Al termine del training, controlla la copia Snapshot di NetApp creata e collegata al lavoro RUN: Ai.
+
....
runai logs dist-lane-detection-training --tail 1
....
+
image::runai-ld_image27.png[runai ld image27]

+
....
kubectl get volumesnapshots | grep download-tusimple-data-0
....






== Ripristinare i dati dalla copia Snapshot di NetApp

Per ripristinare i dati dalla copia Snapshot di NetApp, attenersi alla seguente procedura:

. Passare alla home directory.
+
....
cd ~
....
. Accedere alla directory del progetto `lane-detection-SCNN-horovod`.
+
....
cd ./lane-detection-SCNN-horovod
....
. Modificare `restore-snaphot-pvc.yaml` e aggiornare `dataSource` `name` Nella copia Snapshot da cui si desidera ripristinare i dati. È anche possibile modificare il nome PVC in cui verranno ripristinati i dati, in questo esempio ITS `restored-tusimple`.
+
image::runai-ld_image29.png[runai ld image29]

. Creare un nuovo PVC utilizzando `restore-snapshot-pvc.yaml`.
+
....
kubectl create -f restore-snapshot-pvc.yaml
....
+
L'output dovrebbe essere simile al seguente esempio:

+
image::runai-ld_image30.png[runai ld image30]

. Se si desidera utilizzare i dati appena ripristinati per la formazione, l'invio del lavoro rimane lo stesso di prima; sostituire solo `PVC_NAME` con il ripristinato `PVC_NAME` quando si invia il lavoro di formazione, come indicato nei seguenti comandi:
+
....
runai submit-mpi
--name dist-lane-detection-training
--large-shm
--processes=3
--gpu 1
--pvc restored-tusimple:/mnt
--image muneer7589/dist-lane-detection:3.1
-e USE_WORKERS="true"
-e NUM_WORKERS=4
-e BATCH_SIZE=33
-e USE_VAL="false"
-e VAL_BATCH_SIZE=99
-e ENABLE_SNAPSHOT="true"
-e PVC_NAME="restored-tusimple"
....




== Valutazione delle performance

Per mostrare la scalabilità lineare della soluzione, sono stati eseguiti test delle performance per due scenari: Una GPU e tre GPU. L'allocazione della GPU, l'utilizzo della GPU e della memoria, diverse metriche a nodo singolo e a tre nodi sono state acquisite durante il training sul set di dati di rilevamento della corsia TuSimple. I dati vengono aumentati di cinque volte solo per analizzare l'utilizzo delle risorse durante i processi di training.

La soluzione consente ai clienti di iniziare con un piccolo set di dati e poche GPU. Quando la quantità di dati e la domanda di GPU aumentano, i clienti possono scalare dinamicamente i terabyte nel Tier Standard e scalare rapidamente fino al Tier Premium per ottenere un throughput quattro volte superiore per terabyte senza spostare alcun dato. Questo processo viene spiegato ulteriormente nella sezione, link:runai-ld_lane_detection_distributed_training_with_run_ai.html#azure-netapp-files-service-levels["Livelli di servizio Azure NetApp Files"].

Il tempo di elaborazione su una GPU era di 12 ore e 45 minuti. Il tempo di elaborazione su tre GPU su tre nodi era di circa 4 ore e 30 minuti.

Le figure mostrate nel resto di questo documento illustrano esempi di performance e scalabilità in base alle singole esigenze aziendali.

La figura seguente illustra l'allocazione e l'utilizzo della memoria di 1 GPU.

image::runai-ld_image31.png[runai ld image31]

La figura seguente illustra l'utilizzo della GPU a nodo singolo.

image::runai-ld_image32.png[runai ld image32]

La figura seguente illustra le dimensioni della memoria a nodo singolo (16 GB).

image::runai-ld_image33.png[runai ld image33]

La figura seguente illustra il numero di GPU a nodo singolo (1).

image::runai-ld_image34.png[runai ld image34]

La figura seguente illustra l'allocazione della GPU a nodo singolo (%).

image::runai-ld_image35.png[runai ld image35]

La figura seguente illustra tre GPU su tre nodi: Allocazione e memoria delle GPU.

image::runai-ld_image36.png[runai ld image36]

La figura seguente illustra tre GPU in tre nodi utilizzati (%).

image::runai-ld_image37.png[runai ld image37]

La figura seguente illustra tre GPU in tre nodi di utilizzo della memoria (%).

image::runai-ld_image38.png[runai ld image38]



== Livelli di servizio Azure NetApp Files

È possibile modificare il livello di servizio di un volume esistente spostando il volume in un altro pool di capacità che utilizza https://docs.microsoft.com/azure/azure-netapp-files/azure-netapp-files-service-levels["livello di servizio"^] si desidera per il volume. Questa modifica del livello di servizio esistente per il volume non richiede la migrazione dei dati. Inoltre, non influisce sull'accesso al volume.



=== Modificare dinamicamente il livello di servizio di un volume

Per modificare il livello di servizio di un volume, attenersi alla seguente procedura:

. Nella pagina Volumes (volumi), fare clic con il pulsante destro del mouse sul volume di cui si desidera modificare il livello di servizio. Selezionare Cambia pool.
+
image::runai-ld_image39.png[runai ld image39]

. Nella finestra Change Pool, selezionare il pool di capacità in cui si desidera spostare il volume. Quindi, fare clic su OK.
+
image::runai-ld_image40.png[runai ld image40]





=== Automatizzare la modifica del livello di servizio

La modifica dinamica del livello di servizio è ancora in Public Preview, ma non è attivata per impostazione predefinita. Per attivare questa funzione nell'abbonamento Azure, seguire la procedura descritta nel documento " file:///C:\Users\crich\Downloads\•%09https:\docs.microsoft.com\azure\azure-netapp-files\dynamic-change-volume-service-level["Modificare dinamicamente il livello di servizio di un volume"^]."

* Per Azure è inoltre possibile utilizzare i seguenti comandi: CLI. Per ulteriori informazioni su come modificare le dimensioni del pool di Azure NetApp Files, visitare il sito https://docs.microsoft.com/cli/azure/netappfiles/volume?view=azure-cli-latest-az_netappfiles_volume_pool_change["Volume netappfiles az: Gestione delle risorse dei volumi ANF (Azure NetApp Files)"^].
+
....
az netappfiles volume pool-change -g mygroup
--account-name myaccname
-pool-name mypoolname
--name myvolname
--new-pool-resource-id mynewresourceid
....
* Il `set- aznetappfilesvolumepool` Il cmdlet illustrato può modificare il pool di un volume Azure NetApp Files. Per ulteriori informazioni sulla modifica delle dimensioni del pool di volumi e di Azure PowerShell, visitare il sito Web https://docs.microsoft.com/powershell/module/az.netappfiles/set-aznetappfilesvolumepool?view=azps-5.8.0["Modifica del pool per un volume Azure NetApp Files"^].
+
....
Set-AzNetAppFilesVolumePool
-ResourceGroupName "MyRG"
-AccountName "MyAnfAccount"
-PoolName "MyAnfPool"
-Name "MyAnfVolume"
-NewPoolResourceId 7d6e4069-6c78-6c61-7bf6-c60968e45fbf
....

