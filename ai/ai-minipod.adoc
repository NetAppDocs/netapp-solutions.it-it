---
sidebar: sidebar 
permalink: ai/ai-minipod.html 
keywords: netapp, aipod, RAG, ai solution, design 
summary: 'Questo articolo presenta un progetto di riferimento convalidato di NetApp® AIPod per Enterprise RAG, con tecnologie e funzionalità combinate di processori Intel® Xeon® 6 e soluzioni di gestione dati NetApp. La soluzione illustra un"applicazione ChatQnA downstream che sfrutta un ampio modello linguistico, fornendo risposte accurate e contestualmente rilevanti agli utenti simultanei. Le risposte vengono recuperate dal repository di conoscenze interno di un"organizzazione tramite una pipeline di inferenza RAG air-gapped.' 
---
= NetApp AIPod Mini - Inferenza RAG aziendale con NetApp e Intel
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Questo articolo presenta un progetto di riferimento convalidato di NetApp® AIPod per Enterprise RAG, con tecnologie e funzionalità combinate di processori Intel® Xeon® 6 e soluzioni di gestione dati NetApp. La soluzione illustra un'applicazione ChatQnA downstream che sfrutta un ampio modello linguistico, fornendo risposte accurate e contestualmente rilevanti agli utenti simultanei. Le risposte vengono recuperate dal repository di conoscenze interno di un'organizzazione tramite una pipeline di inferenza RAG air-gapped.

image:aipod-mini-image01.png["100.100"]

Sathish Thyagarajan, Michael Oglesby, NetApp



== Sintesi

Un numero crescente di organizzazioni sfrutta le applicazioni di generazione aumentata dal recupero (RAG) e i modelli linguistici di grandi dimensioni (LLM) per interpretare i prompt degli utenti e generare risposte per aumentare la produttività e il valore aziendale. Questi prompt e risposte possono includere testo, codice, immagini o persino strutture proteiche terapeutiche recuperate dalla knowledge base interna di un'organizzazione, da data lake, repository di codice e repository di documenti. Questo documento illustra il progetto di riferimento della soluzione NetApp® AIPod™ Mini, che comprende storage NetApp AFF e server con processori Intel® Xeon® 6. Include il software di gestione dati NetApp ONTAP® combinato con Intel® Advanced Matrix Extensions (Intel® AMX) e il software Intel® AI for Enterprise Retrieval-augmented Generation (RAG) basato su Open Platform for Enterprise AI (OPEA). NetApp AIPod Mini per RAG aziendale consente alle organizzazioni di trasformare un LLM pubblico in una soluzione di inferenza di intelligenza artificiale generativa (GenAI) privata. La soluzione dimostra un'inferenza RAG efficiente e conveniente su scala aziendale, progettata per migliorare l'affidabilità e fornirti un controllo migliore sulle tue informazioni proprietarie.



== Convalida del partner di storage Intel

I server basati su processori Intel Xeon 6 sono progettati per gestire carichi di lavoro di inferenza AI impegnativi, utilizzando Intel AMX per le massime prestazioni. Per garantire prestazioni di storage e scalabilità ottimali, la soluzione è stata convalidata con successo utilizzando NetApp ONTAP, consentendo alle aziende di soddisfare le esigenze delle applicazioni RAG. Questa convalida è stata condotta su server dotati di processori Intel Xeon 6. Intel e NetApp vantano una solida partnership incentrata sulla fornitura di soluzioni AI ottimizzate, scalabili e in linea con i requisiti aziendali dei clienti.



== Vantaggi dell'esecuzione di sistemi RAG con NetApp

Le applicazioni RAG prevedono il recupero di informazioni dai repository di documenti aziendali in vari formati, come PDF, testo, CSV, Excel o knowledge graph. Questi dati vengono solitamente archiviati in soluzioni come l'archiviazione di oggetti S3 o NFS on-premise come origine dei dati. NetApp è leader nelle tecnologie di gestione, mobilità, governance e sicurezza dei dati nell'ecosistema edge, data center e cloud. La gestione dei dati NetApp ONTAP fornisce storage di livello enterprise per supportare vari tipi di carichi di lavoro di intelligenza artificiale, tra cui inferenza batch e in tempo reale, e offre alcuni dei seguenti vantaggi:

* Velocità e scalabilità. È possibile gestire grandi set di dati ad alta velocità per il versioning, con la possibilità di scalare prestazioni e capacità in modo indipendente.
* Accesso ai dati. Il supporto multiprotocollo consente alle applicazioni client di leggere i dati utilizzando i protocolli di condivisione file S3, NFS e SMB. I bucket NAS S3 di ONTAP possono facilitare l'accesso ai dati in scenari di inferenza LLM multimodali.
* Affidabilità e riservatezza. ONTAP offre protezione dei dati, protezione autonoma da ransomware (ARP) NetApp integrata e provisioning dinamico dello storage, e offre crittografia sia software che hardware per migliorare riservatezza e sicurezza. ONTAP è conforme allo standard FIPS 140-2 per tutte le connessioni SSL.




== Pubblico di riferimento

Questo documento è destinato a decision maker, data engineer, dirigenti aziendali e dirigenti di reparto che desiderano sfruttare un'infrastruttura progettata per fornire soluzioni RAG e GenAI aziendali. Una conoscenza approfondita dell'inferenza AI, dei LLM, di Kubernetes e del networking e dei suoi componenti sarà utile durante la fase di implementazione.



== Requisiti tecnologici



=== Hardware



==== Tecnologie di intelligenza artificiale Intel

Con Xeon 6 come CPU host, i sistemi accelerati beneficiano di elevate prestazioni single-thread, maggiore larghezza di banda di memoria, maggiore affidabilità, disponibilità e facilità di manutenzione (RAS) e più canali di I/O. Intel AMX accelera l'inferenza per INT8 e BF16 e offre supporto per i modelli addestrati per FP16, con un massimo di 2.048 operazioni in virgola mobile per ciclo per core per INT8 e 1.024 operazioni in virgola mobile per ciclo per core per BF16/FP16. Per implementare una soluzione RAG utilizzando processori Xeon 6, si consiglia generalmente una RAM minima di 250 GB e 500 GB di spazio su disco. Tuttavia, questo dipende fortemente dalle dimensioni del modello LLM. Per ulteriori informazioni, fare riferimento a Intel  https://www.intel.com/content/dam/www/central-libraries/us/en/documents/2024-05/intel-xeon-6-product-brief.pdf["Processore Xeon 6"^] descrizione del prodotto.

Figura 1 - Server di elaborazione con processori Intel Xeon 6 image:aipod-mini-image02.png["300.300"]



==== Archiviazione AFF NetApp

I sistemi NetApp AFF A-Series di fascia entry-level e mid-level offrono prestazioni più potenti, densità e maggiore efficienza. I sistemi NetApp AFF A20, AFF A30 e AFF A50 offrono un vero storage unificato che supporta blocchi, file e oggetti, basato su un unico sistema operativo in grado di gestire, proteggere e mobilitare i dati per le applicazioni RAG al costo più basso possibile nel cloud ibrido.

Figura 2 - Sistema NetApp AFF A-Series. image:aipod-mini-image03.png["300.300"]

|===
| *Hardware* | *Quantità* | *Commento* 


| Server basato su Intel Xeon 6 | 2 | Nodi di inferenza RAG: con processori Intel Xeon serie 6900 o Intel Xeon serie 6700 a doppio socket e da 250 GB a 3 TB di RAM con DDR5 (6400 MHz) o MRDIMM (8800 MHz). Server 2U. 


| Server del piano di controllo con processore Intel | 1 | Piano di controllo Kubernetes/server 1U. 


| Scelta dello switch Ethernet da 100 Gb | 1 | Switch del centro dati. 


| NetApp AFF A20 (o AFF A30; AFF A50) | 1 | Capacità di archiviazione massima: 9,3 PB. Nota: rete: porte 10/25/100 GbE. 
|===
Per la convalida di questo progetto di riferimento, sono stati utilizzati server con processori Intel Xeon 6 di Supermicro (222HA-TN-OTO-37) e uno switch 100GbE di Arista (7280R3A).



=== Software



==== Piattaforma aperta per l'intelligenza artificiale aziendale

L'Open Platform for Enterprise AI (OPEA) è un'iniziativa open source guidata da Intel in collaborazione con i partner dell'ecosistema. Fornisce una piattaforma modulare di blocchi costitutivi componibili, progettata per accelerare lo sviluppo di sistemi di intelligenza artificiale generativa all'avanguardia, con particolare attenzione ai RAG. OPEA include un framework completo che include LLM, datastore, motori di prompt, blueprint architetturali RAG e un metodo di valutazione in quattro fasi che valuta i sistemi di intelligenza artificiale generativa in base a prestazioni, funzionalità, affidabilità e preparazione aziendale.

L'OPEA è composta essenzialmente da due componenti chiave:

* GenAIComps: un toolkit basato sui servizi composto da componenti di microservizi
* GenAIExamples: soluzioni pronte per l'uso come ChatQnA che dimostrano casi d'uso pratici


Per maggiori dettagli, vedere il  https://opea-project.github.io/latest/index.html["Documentazione del progetto OPEA"^]



==== Inferenza Intel AI for Enterprise basata su OPEA

OPEA per Intel AI for Enterprise RAG semplifica la trasformazione dei dati aziendali in informazioni fruibili. Basato su processori Intel Xeon, integra componenti di partner del settore per offrire un approccio semplificato all'implementazione di soluzioni aziendali. Si adatta perfettamente a framework di orchestrazione collaudati, offrendo la flessibilità e la possibilità di scelta di cui la tua azienda ha bisogno.

Basandosi sulle fondamenta di OPEA, Intel AI for Enterprise RAG amplia questa base con funzionalità chiave che migliorano scalabilità, sicurezza ed esperienza utente. Queste funzionalità includono funzionalità di service mesh per una perfetta integrazione con le moderne architetture basate sui servizi, convalida pronta per la produzione per l'affidabilità della pipeline e un'interfaccia utente ricca di funzionalità per RAG come servizio, che consente una facile gestione e monitoraggio dei flussi di lavoro. Inoltre, il supporto di Intel e dei partner fornisce accesso a un ampio ecosistema di soluzioni, combinato con la gestione integrata di identità e accessi (IAM) con interfaccia utente e applicazioni per operazioni sicure e conformi. Guardrail programmabili forniscono un controllo granulare sul comportamento della pipeline, consentendo impostazioni di sicurezza e conformità personalizzate.



==== NetApp ONTAP

NetApp ONTAP è la tecnologia fondamentale alla base delle soluzioni di storage dati critici di NetApp. ONTAP include diverse funzionalità di gestione e protezione dei dati, come la protezione automatica da ransomware contro gli attacchi informatici, funzionalità integrate di trasporto dati e funzionalità di efficienza dello storage. Questi vantaggi si applicano a una vasta gamma di architetture, da quelle on-premise a quelle multicloud ibride in NAS, SAN, storage a oggetti e software-defined per distribuzioni LLM. È possibile utilizzare un server di storage a oggetti ONTAP S3 in un cluster ONTAP per distribuire applicazioni RAG, sfruttando l'efficienza e la sicurezza dello storage di ONTAP, fornite tramite utenti autorizzati e applicazioni client. Per ulteriori informazioni, fare riferimento a. https://docs.netapp.com/us-en/ontap/s3-config/index.html["Informazioni sulla configurazione di ONTAP S3"^]



==== Trident di NetApp

Il software NetApp Trident™ è un orchestratore di storage open source e completamente supportato per container e distribuzioni Kubernetes, tra cui Red Hat OpenShift. Trident funziona con l'intero portfolio di storage NetApp, incluso NetApp ONTAP, e supporta anche connessioni NFS e iSCSI. Per ulteriori informazioni, fare riferimento a. https://github.com/NetApp/trident["NetApp Trident su Git"^]

|===
| *Software* | *Versione* | *Commento* 


| OPEA per Intel AI per aziende RAG | 1.1.2 | Piattaforma RAG aziendale basata sui microservizi OPEA 


| Interfaccia di archiviazione del contenitore (driver CSI) | NetApp Trident 25.02 | Abilita il provisioning dinamico, le copie NetApp Snapshot™ e i volumi. 


| Ubuntu | 22.04.5 | Sistema operativo su cluster a due nodi 


| Orchestrazione di container | Kubernetes 1.31.4 | Ambiente per eseguire il framework RAG 


| ONTAP | ONTAP 9.16.1P4 | Sistema operativo di archiviazione su AFF A20. Include Vscan e ARP. 
|===


== Implementazione della soluzione



=== Stack software

La soluzione è implementata su un cluster Kubernetes composto da nodi applicativi basati su Intel Xeon. Sono necessari almeno tre nodi per implementare l'alta disponibilità di base per il piano di controllo di Kubernetes. Abbiamo convalidato la soluzione utilizzando il seguente layout di cluster.

Tabella 3 - Layout del cluster Kubernetes

|===
| Nodo | Ruolo | Quantità 


| Server con processori Intel Xeon 6 e 1 TB di RAM | Nodo app, nodo piano di controllo | 2 


| Server generico | Nodo del piano di controllo | 1 
|===
La figura seguente illustra una “vista dello stack software” della soluzione. image:aipod-mini-image04.png["600.600"]



=== Fasi di implementazione



==== Distribuisci l'appliance di archiviazione ONTAP

Distribuisci e predisponi il tuo dispositivo di storage NetApp ONTAP. Per ulteriori informazioni, fare riferimento alla https://docs.netapp.com/us-en/ontap-systems-family/["Documentazione sui sistemi hardware ONTAP"^] .



==== Configurare un ONTAP SVM per l'accesso NFS e S3

Configurare una macchina virtuale di archiviazione ONTAP (SVM) per l'accesso NFS e S3 su una rete accessibile dai nodi Kubernetes.

Per creare una SVM utilizzando ONTAP System Manager, accedere a Storage > Storage VM e fare clic sul pulsante + Aggiungi. Quando si abilita l'accesso S3 per la SVM, scegliere l'opzione per utilizzare un certificato firmato da una CA esterna (autorità di certificazione), non un certificato generato dal sistema. È possibile utilizzare un certificato autofirmato o un certificato firmato da una CA pubblicamente attendibile. Per ulteriori dettagli, fare riferimento a  https://docs.netapp.com/us-en/ontap/index.html["Documentazione ONTAP."^]

Lo screenshot seguente illustra la creazione di una SVM utilizzando ONTAP System Manager. Modificare i dettagli secondo necessità in base al proprio ambiente.

Figura 4 - Creazione di SVM tramite ONTAP System Manager. image:aipod-mini-image05.png["600.600"]image:aipod-mini-image06.png["600.600"]



==== Configurare le autorizzazioni S3

Configurare le impostazioni utente/gruppo S3 per la SVM creata nel passaggio precedente. Assicurarsi di disporre di un utente con accesso completo a tutte le operazioni API S3 per quella SVM. Per i dettagli, consultare la documentazione di ONTAP S3.

Nota: questo utente sarà necessario per il servizio di acquisizione dati dell'applicazione Intel AI for Enterprise RAG. Se hai creato la tua SVM utilizzando ONTAP System Manager, quest'ultimo avrà creato automaticamente un utente denominato  `sm_s3_user` e una politica denominata  `FullAccess` quando hai creato il tuo SVM, ma non ti saranno state assegnate autorizzazioni  `sm_s3_user` .

Per modificare le autorizzazioni per questo utente, vai su Archiviazione > VM di archiviazione, fai clic sul nome dell'SVM creato nel passaggio precedente, fai clic su Impostazioni, quindi fai clic sull'icona della matita accanto a "S3". Per assegnare  `sm_s3_user` accesso completo a tutte le operazioni API S3, crea un nuovo gruppo che associa  `sm_s3_user` con il  `FullAccess` politica come illustrato nello screenshot seguente.

Figura 5 - Autorizzazioni S3.

image:aipod-mini-image07.png["600.600"]



==== Creare un bucket S3

Crea un bucket S3 all'interno della SVM creata in precedenza. Per creare una SVM utilizzando ONTAP System Manager, vai a Storage > Bucket e fai clic sul pulsante + Aggiungi. Per ulteriori dettagli, consulta la documentazione di ONTAP S3.

La seguente schermata illustra la creazione di un bucket S3 utilizzando ONTAP System Manager.

Figura 6 - Creazione di un bucket S3. image:aipod-mini-image08.png["600.600"]



==== Configurare le autorizzazioni del bucket S3

Configura le autorizzazioni per il bucket S3 creato nel passaggio precedente. Assicurati che l'utente configurato nel passaggio precedente disponga delle seguenti autorizzazioni:  `GetObject, PutObject, DeleteObject, ListBucket, GetBucketAcl, GetObjectAcl, ListBucketMultipartUploads, ListMultipartUploadParts, GetObjectTagging, PutObjectTagging, DeleteObjectTagging, GetBucketLocation, GetBucketVersioning, PutBucketVersioning, ListBucketVersions, GetBucketPolicy, PutBucketPolicy, DeleteBucketPolicy, PutLifecycleConfiguration, GetLifecycleConfiguration, GetBucketCORS, PutBucketCORS.`

Per modificare le autorizzazioni del bucket S3 utilizzando ONTAP System Manager, accedere a Storage > Bucket, fare clic sul nome del bucket, quindi su Autorizzazioni e infine su Modifica. Fare riferimento a  https://docs.netapp.com/us-en/ontap/object-storage-management/index.html["Documentazione di ONTAP S3"^] per ulteriori dettagli.

La seguente schermata illustra le autorizzazioni bucket necessarie in ONTAP System Manager.

Figura 7 - Autorizzazioni del bucket S3. image:aipod-mini-image09.png["600.600"]



==== Crea una regola di condivisione delle risorse multiorigine del bucket

Utilizzando l'interfaccia a riga di comando di ONTAP, crea una regola CORS (Cross-Origin Resource Sharing) per il bucket creato in un passaggio precedente:

[source, cli]
----
ontap::> bucket cors-rule create -vserver erag -bucket erag-data -allowed-origins *erag.com -allowed-methods GET,HEAD,PUT,DELETE,POST -allowed-headers *
----
Questa regola consente all'applicazione web OPEA per Intel AI for Enterprise RAG di interagire con il bucket dall'interno di un browser web.



==== Distribuisci server

Distribuisci i tuoi server e installa Ubuntu 22.04 LTS su ogni server. Dopo aver installato Ubuntu, installa le utility NFS su ogni server. Per installare le utility NFS, esegui il seguente comando:

[source, cli]
----
 apt-get update && apt-get install nfs-common
----


==== Installa Kubernetes

Installa Kubernetes sui tuoi server utilizzando Kubespray. Per ulteriori informazioni, fare riferimento alla https://kubespray.io/["Documentazione di Kubespray"^] .



==== Installa il driver Trident CSI

Installa il driver NetApp Trident CSI nel tuo cluster Kubernetes. Per ulteriori informazioni, fare riferimento alla https://docs.netapp.com/us-en/trident/trident-get-started/kubernetes-deploy.html["Documentazione di installazione del Trident"^] .



==== Crea un back-end Trident

Crea un back-end Trident per la SVM creata in precedenza. Quando crei il back-end, usa  `ontap-nas` autista. Per ulteriori informazioni, fare riferimento alla https://docs.netapp.com/us-en/trident/trident-use/ontap-nas.html["Documentazione back-end di Trident"^] .



==== Creare una classe di storage

Crea una classe di storage Kubernetes corrispondente al back-end Trident creato nel passaggio precedente. Per maggiori dettagli, consulta la documentazione relativa alle classi di storage Trident.



==== OPEA per Intel AI per aziende RAG

Installa OPEA per Intel AI per Enterprise RAG nel tuo cluster Kubernetes. Fai riferimento a  https://github.com/opea-project/Enterprise-RAG/blob/release-1.2.0/deployment/README.md["Intel AI per l'implementazione di RAG aziendali"^] Per i dettagli, consultare la documentazione. Assicurarsi di prendere nota delle modifiche necessarie al file di configurazione descritte più avanti in questo documento. È necessario apportare queste modifiche prima di eseguire il playbook di installazione affinché l'applicazione Intel AI for Enterprise RAG funzioni correttamente con il sistema di storage ONTAP.



=== Abilita l'uso di ONTAP S3

Durante l'installazione di OPEA per Intel AI per Enterprise RAG, modificare il file di configurazione principale per abilitare l'uso di ONTAP S3 come repository di dati di origine.

Per abilitare l'uso di ONTAP S3, impostare i seguenti valori all'interno di  `edp` sezione.

Nota: per impostazione predefinita, l'applicazione Intel AI for Enterprise RAG acquisisce dati da tutti i bucket esistenti nella SVM. Se nella SVM sono presenti più bucket, è possibile modificare  `bucketNameRegexFilter` campo in modo che i dati vengano acquisiti solo da determinati bucket.

[source, cli]
----
edp:
  enabled: true
  namespace: edp
  dpGuard:
    enabled: false
  storageType: s3compatible
  s3compatible:
    region: "us-east-1"
    accessKeyId: "<your_access_key>"
    secretAccessKey: "<your_secret_key>"
    internalUrl: "https://<your_ONTAP_S3_interface>"
    externalUrl: "https://<your_ONTAP_S3_interface>"
    bucketNameRegexFilter: ".*"
----


=== Configurare le impostazioni di sincronizzazione pianificata

Durante l'installazione dell'applicazione OPEA per Intel AI for Enterprise RAG, abilitare  `scheduledSync` in modo che l'applicazione acquisisca automaticamente file nuovi o aggiornati dai bucket S3.

Quando  `scheduledSync` è abilitata, l'applicazione controlla automaticamente i bucket S3 di origine per individuare file nuovi o aggiornati. Tutti i file nuovi o aggiornati rilevati durante questo processo di sincronizzazione vengono automaticamente acquisiti e aggiunti alla knowledge base RAG. L'applicazione controlla i bucket di origine in base a un intervallo di tempo preimpostato. L'intervallo di tempo predefinito è di 60 secondi, il che significa che l'applicazione verifica la presenza di modifiche ogni 60 secondi. È possibile modificare questo intervallo in base alle proprie esigenze specifiche.

Per abilitare  `scheduledSync` e impostare l'intervallo di sincronizzazione, impostare i seguenti valori in  `deployment/components/edp/values.yaml:`

[source, cli]
----
celery:
  config:
    scheduledSync:
      enabled: true
      syncPeriodSeconds: "60"
----


=== Modifica le modalità di accesso al volume

In  `deployment/components/gmc/microservices-connector/helm/values.yaml` , per ogni volume nel  `pvc` elenco, cambia il  `accessMode` A  `ReadWriteMany` .

[source, cli]
----
pvc:
  modelLlm:
    name: model-volume-llm
    accessMode: ReadWriteMany
    storage: 100Gi
  modelEmbedding:
    name: model-volume-embedding
    accessMode: ReadWriteMany
    storage: 20Gi
  modelReranker:
    name: model-volume-reranker
    accessMode: ReadWriteMany
    storage: 10Gi
  vectorStore:
    name: vector-store-data
    accessMode: ReadWriteMany
    storage: 20Gi
----


=== (Facoltativo) Disabilita la verifica del certificato SSL

Se hai utilizzato un certificato autofirmato quando hai abilitato l'accesso S3 per la tua SVM, devi disabilitare la verifica del certificato SSL. Se hai utilizzato un certificato firmato da una CA pubblicamente attendibile, puoi saltare questo passaggio.

Per disabilitare la verifica del certificato SSL, impostare i seguenti valori in  `deployment/components/edp/values.yaml:`

[source, cli]
----
edpExternalUrl: "https://s3.erag.com"
edpExternalSecure: "true"
edpExternalCertVerify: "false"
edpInternalUrl: "edp-minio:9000"
edpInternalSecure: "true"
edpInternalCertVerify: "false"
----


==== Accedi a OPEA per Intel AI for Enterprise RAG UI

Accedi all'interfaccia utente OPEA per Intel AI for Enterprise RAG. Per ulteriori informazioni, fare riferimento alla https://github.com/opea-project/Enterprise-RAG/blob/release-1.1.2/deployment/README.md#interact-with-chatqna["Documentazione sulla distribuzione di Intel AI for Enterprise RAG"^] .

Figura 8: OPEA per Intel AI for Enterprise RAG UI. image:aipod-mini-image10.png["600.600"]



==== Ingestione di dati per RAG

Ora puoi importare file da includere nell'aumento delle query basato su RAG. Sono disponibili diverse opzioni per importare i file. Scegli l'opzione più adatta alle tue esigenze.

Nota: dopo l'acquisizione di un file, l'applicazione OPEA per Intel AI for Enterprise RAG verifica automaticamente la presenza di aggiornamenti al file e acquisisce gli aggiornamenti di conseguenza.

*Opzione 1: Caricamento diretto sul bucket S3. Per importare più file contemporaneamente, ti consigliamo di caricarli sul bucket S3 (quello creato in precedenza) utilizzando il client S3 che preferisci. Tra i client S3 più diffusi figurano AWS CLI, Amazon SDK for Python (Boto3), s3cmd, S3 Browser, Cyberduck e Commander One. Se i file sono di un tipo supportato, tutti i file caricati sul bucket S3 verranno automaticamente importati dall'applicazione OPEA per Intel AI for Enterprise RAG.

Nota: al momento in cui scriviamo, sono supportati i seguenti tipi di file: PDF, HTML, TXT, DOC, DOCX, PPT, PPTX, MD, XML, JSON, JSONL, YAML, XLS, XLSX, CSV, TIFF, JPG, JPEG, PNG e SVG.

È possibile utilizzare l'interfaccia utente di OPEA per Intel AI for Enterprise RAG per verificare che i file siano stati correttamente acquisiti. Per maggiori dettagli, consultare la documentazione relativa all'interfaccia utente di Intel AI for Enterprise RAG. Si noti che l'acquisizione di un numero elevato di file potrebbe richiedere del tempo.

*Opzione 2: Caricamento tramite l'interfaccia utente. Se devi importare solo un numero limitato di file, puoi importarli tramite l'interfaccia utente di OPEA per Intel AI for Enterprise RAG. Per maggiori dettagli, consulta la documentazione relativa all'interfaccia utente di Intel AI for Enterprise RAG.

Figura 9 - Interfaccia utente per l'acquisizione dei dati. image:aipod-mini-image11.png["600.600"]



==== Eseguire query di chat

Ora puoi "chattare" con l'applicazione OPEA per Intel AI for Enterprise RAG utilizzando l'interfaccia utente della chat inclusa. Quando risponde alle tue domande, l'applicazione esegue la RAG utilizzando i file ingeriti. Ciò significa che l'applicazione cerca automaticamente le informazioni rilevanti all'interno dei file ingeriti e le integra nelle risposte alle tue domande.



== Guida al dimensionamento

Nell'ambito del nostro lavoro di convalida, abbiamo condotto test prestazionali in collaborazione con Intel. Questi test hanno prodotto le indicazioni di dimensionamento illustrate nella tabella seguente.

|===
| Caratterizzazioni | Valore | Commento 


| Dimensioni del modello | 20 miliardi di parametri | Llama-8B, Llama-13B, Mistral 7B, Qwen 14B, DeepSeek Distill 8B 


| Dimensione di input | ~2k gettoni | ~4 pagine 


| Dimensione dell'output | ~2k gettoni | ~4 pagine 


| utenti simultanei | 32 | Con "utenti simultanei" si intendono richieste rapide che inviano query contemporaneamente. 
|===
Nota: le indicazioni sul dimensionamento presentate sopra si basano sulla convalida delle prestazioni e sui risultati dei test raccolti utilizzando processori Intel Xeon 6 con 96 core. Per i clienti con token I/O e requisiti di dimensioni del modello simili, consigliamo di utilizzare server con processori Xeon 6 con 96 o 128 core.



== Conclusione

I sistemi RAG aziendali e gli LLM sono tecnologie che interagiscono per aiutare le organizzazioni a fornire risposte accurate e contestualizzate. Queste risposte prevedono il recupero di informazioni basato su una vasta raccolta di dati aziendali privati e interni. Utilizzando RAG, API, incorporamenti vettoriali e sistemi di storage ad alte prestazioni per interrogare i repository di documenti contenenti dati aziendali, i dati vengono elaborati in modo più rapido e sicuro. NetApp AIPod Mini combina l'infrastruttura dati intelligente di NetApp con le funzionalità di gestione dei dati ONTAP e i processori Intel Xeon 6, Intel AI for Enterprise RAG e lo stack software OPEA per supportare l'implementazione di applicazioni RAG ad alte prestazioni e indirizzare le organizzazioni verso la leadership nell'intelligenza artificiale.



== Riconoscimento

Questo documento è opera di Sathish Thyagarajan e Michael Ogelsby, membri del team di NetApp Solutions Engineering. Gli autori desiderano inoltre ringraziare il team di prodotto Enterprise AI di Intel – Ajay Mungara, Mikolaj Zyczynski, Igor Konopko, Ramakrishna Karamsetty, Michal Prostko, Shreejan Mistry e Ned Fiori – e gli altri membri del team NetApp – Lawrence Bunka, Bobby Oommen e Jeff Liborio – per il loro continuo supporto e aiuto durante la convalida di questa soluzione.



== Distinta base

Di seguito è riportato il BOM utilizzato per la convalida funzionale di questa soluzione e può essere utilizzato come riferimento. È possibile utilizzare qualsiasi server o componente di rete (o anche una rete esistente con larghezza di banda preferibilmente di 100 GbE) che si allinei alla seguente configurazione.

Per il server dell'app:

|===
| *Codice parte* | *Descrizione del prodotto* | *Quantità* 


| 222HA-TN-OTO-37 | Hyper SuperServer SYS-222HA-TN /2U | 2 


| P4X-GNR6972P-SRPL2-UCC | Processore Intel Xeon 6972P 2P 128C 2G 504M 500W SGX512 | 2 


| RAM | MEM-DR564MC-ER64(x16)64GB DDR5-6400 2RX4 (16Gb) ECC RDIMM | 32 


|  | HDS-M2N4-960G0-E1-TXD-NON-080(x2) SSD M.2 NVMe PCIe4 960 GB 1DWPD TLC D, 80 mm | 2 


|  | Alimentatore ridondante WS-1K63A-1R(x2)1U da 692W/1600W a singola uscita. Dissipazione termica di 2361 BTU/ora con temperatura massima di circa 59 °C. | 4 
|===
Per il server di controllo:

|===


| *Codice parte* | *Descrizione del prodotto* | *Quantità* 


| 511R-M-OTO-17 | OTTIMIZZATO FINO A 1U X13SCH-SYS, CSE-813MF2TS-R0RCNBP, PWS-602A-1R | 1 


| P4X-GNR6972P-SRPL2-UCC | P4D-G7400-SRL66(x1) ADL Pentium G7400 | 1 


| RAM | MEM-DR516MB-EU48(x2)UDIMM ECC DDR5-4800 1Rx8 (16Gb) da 16 GB | 1 


|  | HDS-M2N4-960G0-E1-TXD-NON-080(x2) SSD M.2 NVMe PCIe4 960 GB 1DWPD TLC D, 80 mm | 2 
|===
Per lo switch di rete:

|===


| *Codice parte* | *Descrizione del prodotto* | *Quantità* 


| DCS-7280CR3A | Arista 7280R3A 28x100 GbE | 1 
|===
Archiviazione AFF NetApp:

|===


| *Codice parte* | *Descrizione del prodotto* | *Quantità* 


| AFF-A20A-100-C | Sistema AFF A20 HA, -C | 1 


| X800-42U-R6-C | Jumper Crd, in cabina, C13-C14, -C | 2 


| X97602A-C | Alimentatore, 1600W, Titanio, -C | 2 


| X66211B-2-N-C | Cavo, 100 GbE, QSFP28-QSFP28, Cu, 2 m, -C | 4 


| X66240A-05-N-C | Cavo, 25 GbE, SFP28-SFP28, Cu, 0,5 m, -C | 2 


| X5532A-N-C | Binario, 4 pali, sottile, foro rotondo/quadrato, piccolo, regolabile, 24-32, -C | 1 


| X4024A-2-A-C | Unità Pack 2X1,92 TB, NVMe4, SED, -C | 6 


| X60130A-C | Modulo IO, 2PT, 100 GbE, -C | 2 


| X60132A-C | Modulo IO, 4PT, 10/25 GbE, -C | 2 


| SW-ONTAPB-FLASH-A20-C | SW, pacchetto base ONTAP, per TB, Flash, A20, -C | 23 
|===


== Dove trovare ulteriori informazioni

Per ulteriori informazioni sulle informazioni descritte in questo documento, consultare i seguenti documenti e/o siti Web:

https://www.netapp.com/support-and-training/documentation/ONTAP%20S3%20configuration%20workflow/["Documentazione sui prodotti NetApp"^]

link:https://github.com/opea-project/Enterprise-RAG/tree/main["Progetto OPEA"]

https://github.com/opea-project/Enterprise-RAG/tree/main/deployment/playbooks["Manuale di distribuzione OPEA Enterprise RAG"^]
