---
sidebar: sidebar 
permalink: databases/automation_ora_c-series_nfs.html 
keywords: Database, Oracle, Azure, ANF, Ansible, Automation 
summary: Questa soluzione offre una panoramica e dettagli per la distribuzione automatizzata di Oracle in NetApp AFF C-Series come storage di database primario con protocollo NFS. Il database Oracle può essere implementato come database container con DNFS attivato. 
---
= TR-4992: Implementazione semplificata e automatizzata di Oracle su NetApp C-Series con NFS
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


Allen Cao, Niyaz Mohamed, NetApp

[role="lead"]
Questa soluzione offre una panoramica e dettagli per la distribuzione automatizzata di Oracle in NetApp AFF C-Series come storage di database primario con protocollo NFS. Il database Oracle può essere implementato come database container con DNFS attivato.



== Scopo

NetApp AFF C-Series è uno storage flash a elevata capacità che rende lo storage all-flash più accessibile e conveniente per lo storage unificato. È sufficiente migliorare le performance per molti carichi di lavoro di database Oracle Tier 1 o Tier 2. Basati sul software di gestione dei dati NetApp ONTAP®, i sistemi AFF C-Series offrono efficienza leader del settore, flessibilità superiore, servizi dati all'avanguardia e integrazione cloud per aiutarvi a scalare la vostra infrastruttura IT, semplificare la gestione dei dati e ridurre i costi di storage e il consumo di energia.

Questa documentazione dimostra l'implementazione semplificata dei database Oracle in NetApp C-Series tramite mount NFS attraverso l'automazione Ansible. Il database Oracle può essere implementato in una configurazione di database container (CDB) e database inseribili (PDB) con il protocollo Oracle DNFS per migliorare le performance. Inoltre, la soluzione offre le Best practice per la configurazione di storage networking e storage virtual machine (SVM) con protocollo NFS su storage controller C-Series. La soluzione include inoltre informazioni su backup rapido dei database Oracle, ripristino e clonazione con il tool UI NetApp SnapCenter.

Questa soluzione risolve i seguenti casi di utilizzo:

* Implementazione automatizzata del database dei container Oracle su storage controller NetApp C-Series.
* Protezione del database Oracle e clone su C-Series con il tool dell'interfaccia utente SnapCenter.




== Pubblico

Questa soluzione è destinata alle seguenti persone:

* Un DBA che vorrebbe implementare Oracle su NetApp C-Series.
* Un Solution Architect per database che vorrebbe testare i carichi di lavoro Oracle su NetApp C-Series.
* Un amministratore dello storage che desidera implementare e gestire un database Oracle su NetApp C-Series.
* Un proprietario di applicazioni che desidera creare un database Oracle su NetApp C-Series.




== Ambiente di test e convalida della soluzione

Il test e la convalida di questa soluzione sono stati eseguiti in un laboratorio che potrebbe non corrispondere all'ambiente di distribuzione finale. Vedere la sezione <<Fattori chiave per l'implementazione>> per ulteriori informazioni.



=== Architettura

image::automation_ora_c-series_nfs_archit.png[Questa immagine fornisce un quadro dettagliato della configurazione di implementazione di Oracle nel cloud pubblico AWS con iSCSI e ASM.]



=== Componenti hardware e software

[cols="33%, 33%, 33%"]
|===


3+| *Hardware* 


| NetApp serie C C400 | ONTAP versione 9.13.1P3 | Due shelf di dischi / 24 dischi con capacità 278 TiB 


| VM per server DB | 4 vCPU, 16GiB GB di RAM | Due istanze di macchine virtuali Linux per l'implementazione simultanea 


| VM per SnapCenter | 4 vCPU, 16GiB GB di RAM | Un'istanza di macchina virtuale Windows 


3+| *Software* 


| RedHat Linux | RHEL Linux 8,6 (LVM) - x64 Gen2 | Implementazione dell'abbonamento a RedHat per il test 


| Server Windows | 2022 DataCenter x64 Gen2 | Server SnapCenter di hosting 


| Database Oracle | Versione 19.18 | Patch RU applicata p34765931_190000_Linux-x86-64.zip 


| Oracle OPatch | Versione 12.2.0.1.36 | Ultima patch p6880880_190000_Linux-x86-64.zip 


| Server SnapCenter | Versione 5,0 | Distribuzione di gruppi di lavoro 


| Aprire JDK | Versione java-11-openjdk | Requisito del plugin SnapCenter per macchine virtuali DB 


| NFS | Versione 3.0 | Oracle DNFS abilitato 


| Ansible | nucleo 2.16.2 | Python 3.6.8 
|===


=== Configurazione del database Oracle nell'ambiente di laboratorio

[cols="33%, 33%, 33%"]
|===


3+|  


| *Server* | *Database* | *Archiviazione DB* 


| ora_01 | NTAP1 (NTAP1_PDB1,NTAP1_PDB2,NTAP1_PDB3) | /U01, /U02, /U03 montaggi NFS su volumi C400 


| ora_02 | NTAP2 (NTAP2_PDB1,NTAP2_PDB2,NTAP2_PDB3) | /U01, /U02, /U03 montaggi NFS su volumi C400 
|===


=== Fattori chiave per l'implementazione

* *Layout dello storage dei database Oracle.* in questa distribuzione automatizzata di Oracle, vengono forniti tre volumi di database per ciascun database per l'hosting di file binari, dati e registri Oracle per impostazione predefinita. I volumi sono montati sul server Oracle DB come /U01 - binario, /U02 - dati, /U03 - registri tramite NFS. I file di controllo doppi sono configurati sui punti di montaggio /U02 e /U03 per la ridondanza.
* *Implementazione di più server DB.* la soluzione di automazione può implementare un database container Oracle su più server DB in un singolo playbook Ansible. Indipendentemente dal numero di server di DB, l'esecuzione del playbook rimane invariata. È possibile implementare più database di container in una singola istanza di macchina virtuale ripetendo la distribuzione con diversi ID di istanze di database (Oracle SID). Tuttavia, assicurarsi che l'host disponga di memoria sufficiente per supportare i database distribuiti.
* *Configurazione DNFS.* utilizzando DNFS (disponibile da Oracle 11g), un database Oracle in esecuzione su una VM DB può gestire un numero di i/o significativamente maggiore rispetto al client NFS nativo. L'implementazione automatizzata di Oracle configura DNFS su NFSv3 per impostazione predefinita.
* *Bilanciamento del carico sulla coppia di controller C400.* posizionare i volumi del database Oracle su C400 nodi di controller in modo uniforme per bilanciare il carico di lavoro. DB1 sul controller 1, DB2 sul controller 2, e così via. Montare i volumi DB sul proprio indirizzo lif locale.
* *Backup del database.* NetApp fornisce una suite software SnapCenter per il backup, il ripristino e la clonazione del database con un'interfaccia utente intuitiva. NetApp consiglia di implementare questo strumento di gestione per ottenere veloci backup delle snapshot (in meno di un minuto), rapidi ripristini del database e cloni del database.




== Implementazione della soluzione

Le sezioni seguenti forniscono procedure dettagliate per la distribuzione automatizzata di Oracle 19c e informazioni per la protezione del database Oracle e la clonazione dopo la distribuzione.



=== Prerequisiti per l'implementazione

[%collapsible]
====
L'implementazione richiede i seguenti prerequisiti.

. Una coppia di storage controller NetApp C-Series è inserita in rack, impilata e installata e configurata l'ultima versione del sistema operativo ONTAP. Se necessario, consultare la presente guida all'installazione: https://docs.netapp.com/us-en/ontap-systems/c400/install-detailed-guide.html#step-1-prepare-for-installation["Guida dettagliata - AFF C400"^]
. Provisioning di due macchine virtuali Linux come server Oracle DB. Per ulteriori informazioni sulla configurazione dell'ambiente, fare riferimento al diagramma dell'architettura riportato nella sezione precedente.
. Eseguire il provisioning di un server Windows per eseguire lo strumento dell'interfaccia utente di NetApp SnapCenter con la versione più recente. Fare riferimento al seguente link per i dettagli: link:https://docs.netapp.com/us-en/snapcenter/install/task_install_the_snapcenter_server_using_the_install_wizard.html["Installare il server SnapCenter"^]
. Esegui il provisioning di una VM Linux come nodo di controller Ansible con l'ultima versione di Ansible e Git installata. Fare riferimento al seguente link per i dettagli: link:../automation/getting-started.html["Introduzione all'automazione delle soluzioni NetApp"^] nella sezione -
`Setup the Ansible Control Node for CLI deployments on RHEL / CentOS` oppure
`Setup the Ansible Control Node for CLI deployments on Ubuntu / Debian`.
+
Abilita l'autenticazione a chiave pubblica/privata ssh tra il controller Ansible e le macchine virtuali di database.

. Dalla home directory dell'utente amministrativo del controller Ansible, clona una copia del toolkit di automazione dell'implementazione Oracle NetApp per NFS.
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_oracle_deploy_nfs.git
----
. Fase successiva ai file di installazione di Oracle 19c nella directory database VM /tmp/archivio con autorizzazione 777.
+
....
installer_archives:
  - "LINUX.X64_193000_db_home.zip"
  - "p34765931_190000_Linux-x86-64.zip"
  - "p6880880_190000_Linux-x86-64.zip"
....


====


=== Configurare Networking e SVM su C-Series per Oracle

[%collapsible]
====
Questa sezione della guida all'implementazione illustra le Best practice per la configurazione di networking e Storage Virtual Machine (SVM) su controller C-Series per carichi di lavoro Oracle con protocollo NFS e con l'interfaccia utente di ONTAP System Manager.

. Accedere a Gestione di sistema ONTAP per verificare che, dopo l'installazione iniziale del cluster ONTAP, i domini di broadcast siano stati configurati con porte ethernet assegnate correttamente a ciascun dominio. In genere, è necessario disporre di un dominio di broadcast per il cluster, di un dominio di broadcast per la gestione e di un dominio di broadcast per il carico di lavoro, ad esempio i dati.
+
image::automation_ora_c-series_nfs_net_01.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Da RETE - Porte Ethernet, fare clic su `Link Aggregate Group` Per creare una porta a0a del gruppo aggregato di collegamento LACP, che fornisce bilanciamento del carico e failover tra le porte membro nella porta del gruppo aggregato. Sono disponibili 4 porte dati - e0e, e0f, e0g, e0h su C400 controller.
+
image::automation_ora_c-series_nfs_net_02.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Selezionare le porte ethernet del gruppo, `LACP` per la modalità, e. `Port` per la distribuzione del carico.
+
image::automation_ora_c-series_nfs_net_03.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Convalidare la porta LACP a0a creata e il dominio di trasmissione `Data` Ora funziona sulla porta LACP.
+
image::automation_ora_c-series_nfs_net_04.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

+
image::automation_ora_c-series_nfs_net_05.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Da `Ethernet Ports`, fare clic su `VLAN` Per aggiungere una VLAN su ciascun nodo controller per il carico di lavoro Oracle sul protocollo NFS.
+
image::automation_ora_c-series_nfs_net_06.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

+
image::automation_ora_c-series_nfs_net_07.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

+
image::automation_ora_c-series_nfs_net_08.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Effettuare l'accesso ai controller C-Series dall'IP di gestione del cluster tramite ssh per validare la corretta configurazione dei gruppi di failover di rete. ONTAP crea e gestisce automaticamente i gruppi di failover.
+
....

HCG-NetApp-C400-E9U9::> net int failover-groups show
  (network interface failover-groups show)
                                  Failover
Vserver          Group            Targets
---------------- ---------------- --------------------------------------------
Cluster
                 Cluster
                                  HCG-NetApp-C400-E9U9a:e0c,
                                  HCG-NetApp-C400-E9U9a:e0d,
                                  HCG-NetApp-C400-E9U9b:e0c,
                                  HCG-NetApp-C400-E9U9b:e0d
HCG-NetApp-C400-E9U9
                 Data
                                  HCG-NetApp-C400-E9U9a:a0a,
                                  HCG-NetApp-C400-E9U9a:a0a-3277,
                                  HCG-NetApp-C400-E9U9b:a0a,
                                  HCG-NetApp-C400-E9U9b:a0a-3277
                 Mgmt
                                  HCG-NetApp-C400-E9U9a:e0M,
                                  HCG-NetApp-C400-E9U9b:e0M
3 entries were displayed.

....
. Da `STORAGE - Storage VMs`, Fare clic su +Add (Aggiungi) per creare una SVM per Oracle.
+
image::automation_ora_c-series_nfs_svm_01.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Assegna un nome alla tua SVM Oracle, verificalo `Enable NFS` e. `Allow NFS client access`.
+
image::automation_ora_c-series_nfs_svm_02.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Aggiungi policy di esportazione NFS `Default` regole.
+
image::automation_ora_c-series_nfs_svm_03.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Poll `NETWORK INTERFACE`, Immettere l'indirizzo IP su ciascun nodo per gli indirizzi NFS lif.
+
image::automation_ora_c-series_nfs_svm_04.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Convalida che SVM per Oracle è attivo/in esecuzione e lo stato dei cicli di vita NFS è attivo.
+
image::automation_ora_c-series_nfs_svm_05.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

+
image::automation_ora_c-series_nfs_svm_06.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Da `STORAGE-Volumes` Tab per aggiungere volumi NFS per database Oracle.
+
image::automation_ora_c-series_nfs_vol_01.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Indica il volume, assegna la capacità e il livello di performance.
+
image::automation_ora_c-series_nfs_vol_02.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Poll `Access Permission`, scegliere il criterio predefinito creato dal passaggio precedente. Deselezionare `Enable Snapshot Copies` Preferiamo utilizzare SnapCenter per creare snapshot coerenti con le applicazioni.
+
image::automation_ora_c-series_nfs_vol_03.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

. Creare tre volumi DB per ogni server DB: Nome_server_U01 - binario, nome_server_U02 - dati, nome_server_U03 - registri.
+
image::automation_ora_c-series_nfs_vol_04.png[Questa immagine fornisce una schermata per la configurazione del controller serie C.]

+

NOTE: La convenzione di denominazione del volume del database deve seguire rigorosamente il formato indicato sopra per garantire il corretto funzionamento dell'automazione.



La configurazione del controller C-series per Oracle viene completata.

====


=== File dei parametri di automazione

[%collapsible]
====
Il playbook Ansible esegue attività di installazione e configurazione del database con parametri predefiniti. Per questa soluzione di automazione Oracle, esistono tre file di parametri definiti dall'utente che devono essere inseriti dall'utente prima dell'esecuzione del playbook.

* host - definisci gli obiettivi per i quali il playbook di automazione è in esecuzione.
* vars/vars.yml - il file variabile globale che definisce le variabili che si applicano a tutti i target.
* host_vars/host_name.yml - il file di variabile locale che definisce le variabili che si applicano solo a una destinazione denominata. Nel nostro caso d'utilizzo, questi sono i server Oracle DB.


Oltre a questi file di variabili definiti dall'utente, esistono diversi file di variabili predefinite che contengono parametri predefiniti che non richiedono modifiche se non necessario. Nelle sezioni seguenti viene illustrato come configurare i file variabili definiti dall'utente.

====


=== Configurazione dei file dei parametri

[%collapsible]
====
. Destinazione Ansible `hosts` configurazione file:
+
[source, shell]
----
# Enter Oracle servers names to be deployed one by one, follow by each Oracle server public IP address, and ssh private key of admin user for the server.
[oracle]
ora_01 ansible_host=10.61.180.21 ansible_ssh_private_key_file=ora_01.pem
ora_02 ansible_host=10.61.180.23 ansible_ssh_private_key_file=ora_02.pem

----
. Globale `vars/vars.yml` configurazione dei file
+
[source, shell]
----
######################################################################
###### Oracle 19c deployment user configuration variables       ######
###### Consolidate all variables from ONTAP, linux and oracle   ######
######################################################################

###########################################
### ONTAP env specific config variables ###
###########################################

# Prerequisite to create three volumes in NetApp ONTAP storage from System Manager or cloud dashboard with following naming convention:
# db_hostname_u01 - Oracle binary
# db_hostname_u02 - Oracle data
# db_hostname_u03 - Oracle redo
# It is important to strictly follow the name convention or the automation will fail.


###########################################
### Linux env specific config variables ###
###########################################

redhat_sub_username: XXXXXXXX
redhat_sub_password: XXXXXXXX


####################################################
### DB env specific install and config variables ###
####################################################

# Database domain name
db_domain: solutions.netapp.com

# Set initial password for all required Oracle passwords. Change them after installation.
initial_pwd_all: XXXXXXXX

----
. Server DB locale `host_vars/host_name.yml` configurazione come ora_01.yml, ora_02.yml ...
+
[source, shell]
----
# User configurable Oracle host specific parameters

# Enter container database SID. By default, a container DB is created with 3 PDBs within the CDB
oracle_sid: NTAP1

# Enter database shared memory size or SGA. CDB is created with SGA at 75% of memory_limit, MB. The grand total of SGA should not exceed 75% available RAM on node.
memory_limit: 8192

# Local NFS lif ip address to access database volumes
nfs_lif: 172.30.136.68

----


====


=== Esecuzione Playbook

[%collapsible]
====
Nel toolkit di automazione sono presenti un totale di cinque playbook. Ciascuna di esse esegue blocchi di attività diversi e ha scopi diversi.

....
0-all_playbook.yml - execute playbooks from 1-4 in one playbook run.
1-ansible_requirements.yml - set up Ansible controller with required libs and collections.
2-linux_config.yml - execute Linux kernel configuration on Oracle DB servers.
4-oracle_config.yml - install and configure Oracle on DB servers and create a container database.
5-destroy.yml - optional to undo the environment to dismantle all.
....
Sono disponibili tre opzioni per eseguire i playbook con i seguenti comandi.

. Esegui tutti i playbook sull'implementazione in un'unica esecuzione combinata.
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml
----
. Eseguire i playbook uno alla volta con la sequenza numerica da 1 a 4.
+
[source, cli]]
----
ansible-playbook -i hosts 1-ansible_requirements.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 2-linux_config.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 4-oracle_config.yml -u admin -e @vars/vars.yml
----
. Esegui 0-all_playbook.yml con un tag.
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t ansible_requirements
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t linux_config
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t oracle_config
----
. Annullare l'ambiente
+
[source, cli]
----
ansible-playbook -i hosts 5-destroy.yml -u admin -e @vars/vars.yml
----


====


=== Convalida post-esecuzione

[%collapsible]
====
Dopo aver eseguito il playbook, effettua l'accesso alla macchina virtuale del server di Oracle DB per validare l'installazione e la configurazione di Oracle e la creazione di un database di container. Segue un esempio di convalida del database Oracle su DB VM ora_01 o ora_02.

. Convalidare i montaggi NFS
+
....

[admin@ora_01 ~]$ cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Wed Oct 18 19:43:31 2023
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
/dev/mapper/rhel-root   /                       xfs     defaults        0 0
UUID=aff942c4-b224-4b62-807d-6a5c22f7b623 /boot                   xfs     defaults        0 0
/dev/mapper/rhel-swap   none                    swap    defaults        0 0
/root/swapfile swap swap defaults 0 0
172.21.21.100:/ora_01_u01 /u01 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
172.21.21.100:/ora_01_u02 /u02 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
172.21.21.100:/ora_01_u03 /u03 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0


[admin@ora_01 tmp]$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G     0  7.8G   0% /dev/shm
tmpfs                      7.8G   18M  7.8G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rhel-root       44G   28G   17G  62% /
/dev/sda1                 1014M  258M  757M  26% /boot
tmpfs                      1.6G   12K  1.6G   1% /run/user/42
tmpfs                      1.6G  4.0K  1.6G   1% /run/user/1000
172.21.21.100:/ora_01_u01   50G  8.7G   42G  18% /u01
172.21.21.100:/ora_01_u02  200G  384K  200G   1% /u02
172.21.21.100:/ora_01_u03  100G  320K  100G   1% /u03

[admin@ora_02 ~]$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G     0  7.8G   0% /dev/shm
tmpfs                      7.8G   18M  7.8G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rhel-root       44G   28G   17G  63% /
/dev/sda1                 1014M  258M  757M  26% /boot
tmpfs                      1.6G   12K  1.6G   1% /run/user/42
tmpfs                      1.6G  4.0K  1.6G   1% /run/user/1000
172.21.21.101:/ora_02_u01   50G  7.8G   43G  16% /u01
172.21.21.101:/ora_02_u02  200G  320K  200G   1% /u02
172.21.21.101:/ora_02_u03  100G  320K  100G   1% /u03

....
. Convalidare Oracle listener
+
....

[admin@ora_02 ~]$ sudo su
[root@ora_02 admin]# su - oracle
[oracle@ora_02 ~]$ lsnrctl status listener.ntap2

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 29-MAY-2024 12:13:30

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=ora_02.cie.netapp.com)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     LISTENER.NTAP2
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                23-MAY-2024 16:13:03
Uptime                    5 days 20 hr. 0 min. 26 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP2/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ora_02/listener.ntap2/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ora_02.cie.netapp.com)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ora_02.cie.netapp.com)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/product/19.0.0/NTAP2/admin/NTAP2/xdb_wallet))(Presentation=HTTP)(Session=RAW))
Services Summary...
Service "192551f1d7e65fc3e06308b43d0a63ae.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "1925529a43396002e06308b43d0a2d5a.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "1925530776b76049e06308b43d0a49c3.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "NTAP2.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "NTAP2XDB.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "ntap2_pdb1.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "ntap2_pdb2.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "ntap2_pdb3.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
The command completed successfully
[oracle@ora_02 ~]$

....
. Convalidare il database Oracle e DNFS
+
....

[oracle@ora-01 ~]$ cat /etc/oratab
#
# This file is used by ORACLE utilities.  It is created by root.sh
# and updated by either Database Configuration Assistant while creating
# a database or ASM Configuration Assistant while creating ASM instance.

# A colon, ':', is used as the field terminator.  A new line terminates
# the entry.  Lines beginning with a pound sign, '#', are comments.
#
# Entries are of the form:
#   $ORACLE_SID:$ORACLE_HOME:<N|Y>:
#
# The first and second fields are the system identifier and home
# directory of the database respectively.  The third field indicates
# to the dbstart utility that the database should , "Y", or should not,
# "N", be brought up at system boot time.
#
# Multiple entries with the same $ORACLE_SID are not allowed.
#
#
NTAP1:/u01/app/oracle/product/19.0.0/NTAP1:Y


[oracle@ora-01 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu Feb 1 16:37:51 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode, log_mode from v$database;

NAME      OPEN_MODE            LOG_MODE
--------- -------------------- ------------
NTAP1     READ WRITE           ARCHIVELOG

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 NTAP1_PDB1                     READ WRITE NO
         4 NTAP1_PDB2                     READ WRITE NO
         5 NTAP1_PDB3                     READ WRITE NO
SQL> select name from v$datafile;

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP1/system01.dbf
/u02/oradata/NTAP1/sysaux01.dbf
/u02/oradata/NTAP1/undotbs01.dbf
/u02/oradata/NTAP1/pdbseed/system01.dbf
/u02/oradata/NTAP1/pdbseed/sysaux01.dbf
/u02/oradata/NTAP1/users01.dbf
/u02/oradata/NTAP1/pdbseed/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/system01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/sysaux01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/users01.dbf

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP1/NTAP1_pdb2/system01.dbf
/u02/oradata/NTAP1/NTAP1_pdb2/sysaux01.dbf
/u02/oradata/NTAP1/NTAP1_pdb2/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb2/users01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/system01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/sysaux01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/users01.dbf

19 rows selected.

SQL> select name from v$controlfile;

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP1/control01.ctl
/u03/orareco/NTAP1/control02.ctl

SQL> select member from v$logfile;

MEMBER
--------------------------------------------------------------------------------
/u03/orareco/NTAP1/onlinelog/redo03.log
/u03/orareco/NTAP1/onlinelog/redo02.log
/u03/orareco/NTAP1/onlinelog/redo01.log

SQL> select svrname, dirname from v$dnfs_servers;

SVRNAME
--------------------------------------------------------------------------------
DIRNAME
--------------------------------------------------------------------------------
172.21.21.100
/ora_01_u02

172.21.21.100
/ora_01_u03

172.21.21.100
/ora_01_u01


....
. Accedere a Oracle Enterprise Manager Express per convalidare il database.
+
image::automation_ora_c-series_nfs_em_01.png[Questa immagine fornisce la schermata di accesso per Oracle Enterprise Manager Express]

+
image::automation_ora_c-series_nfs_em_02.png[Questa immagine fornisce la vista del database dei container da Oracle Enterprise Manager Express]

+
image::automation_ora_c-series_nfs_em_03.png[Questa immagine fornisce la vista del database dei container da Oracle Enterprise Manager Express]



====


=== Backup, ripristino e cloning di Oracle con SnapCenter

[%collapsible]
====
NetApp consiglia il tool dell'interfaccia utente di SnapCenter per gestire i database Oracle distribuiti in C-Series. Fare riferimento a TR-4979 link:aws_ora_fsx_vmc_guestmount.html#oracle-backup-restore-and-clone-with-snapcenter["Oracle semplificata e autogestita in VMware Cloud su AWS con FSX ONTAP montato sul guest"^] sezione `Oracle backup, restore, and clone with SnapCenter` Per informazioni dettagliate su configurazione di SnapCenter ed esecuzione di flussi di lavoro di backup, ripristino e cloning del database.

====


== Dove trovare ulteriori informazioni

Per ulteriori informazioni sulle informazioni descritte in questo documento, consultare i seguenti documenti e/o siti Web:

* NetApp AFF C-Series
+
link:https://www.netapp.com/pdf.html?item=/media/81583-da-4240-aff-c-series.pdf["https://www.netapp.com/pdf.html?item=/media/81583-da-4240-aff-c-series.pdf"^]

* Soluzioni per database aziendali NetApp
+
link:https://docs.netapp.com/us-en/netapp-solutions/databases/index.html["https://docs.netapp.com/us-en/netapp-solutions/databases/index.html"^]

* Distribuzione di Oracle Direct NFS
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2"^]


